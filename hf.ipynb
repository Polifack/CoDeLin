{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/droca1/.conda/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loaded TokenClassification task with 410 labels: ['10[_]ADJP', '10[_]ADJP[+]QP', '10[_]ADVP', '10[_]NP', '10[_]NP[+]QP', '10[_]NP[_]NP', '10[_]NP[_]NX', '10[_]NP[_]VP', '10[_]PP', '10[_]PP[_]INTJ', '10[_]PRN', '10[_]QP', '10[_]S', '10[_]SBAR', '10[_]SBAR[+]S[_]NP', '10[_]SBAR[_]WHNP', '10[_]S[+]VP', '10[_]S[+]VP[_]NP', '10[_]S[+]VP[_]PRT', '10[_]S[_]NP', '10[_]VP', '10[_]VP[_]NP', '11[_]ADJP', '11[_]ADJP[+]QP', '11[_]ADVP', '11[_]ADVP[_]ADVP', '11[_]NP', '11[_]NP[+]QP', '11[_]NP[_]NP', '11[_]PP', '11[_]PP[_]NP', '11[_]QP', '11[_]S', '11[_]SBAR', '11[_]SBAR[+]S', '11[_]SBAR[+]S[+]VP', '11[_]SBAR[_]WHADVP', '11[_]SBAR[_]WHNP', '11[_]S[+]VP', '11[_]S[_]NP', '11[_]UCP', '11[_]VP', '11[_]VP[_]ADVP', '11[_]VP[_]NP', '11[_]VP[_]PP', '11[_]WHPP', '12[_]ADJP', '12[_]ADVP', '12[_]CONJP', '12[_]NP', '12[_]NP[_]ADVP', '12[_]NP[_]NP', '12[_]PP', '12[_]QP', '12[_]S', '12[_]SBAR[+]S', '12[_]SBAR[+]S[+]VP', '12[_]SBAR[_]WHNP', '12[_]S[+]VP', '12[_]S[+]VP[_]NP', '12[_]S[+]VP[_]VP', '12[_]S[_]NP', '12[_]VP', '12[_]VP[_]ADVP', '12[_]VP[_]NP', '13[_]ADVP', '13[_]NP', '13[_]NP[_]ADJP', '13[_]NX[_]NP', '13[_]PP', '13[_]SBAR[+]S', '13[_]SBAR[_]WHNP', '13[_]S[+]VP', '13[_]VP', '13[_]VP[_]ADVP', '13[_]VP[_]NP', '13[_]VP[_]PRT', '14[_]NP', '14[_]NP[+]QP', '14[_]NP[_]NP', '14[_]PP', '14[_]SBAR[_]WHNP', '14[_]S[+]NP[_]NP', '14[_]S[+]VP', '14[_]VP', '14[_]VP[_]ADVP', '15[_]ADJP', '15[_]NP', '15[_]NP[+]QP', '15[_]PP', '15[_]PRN', '15[_]SBAR', '15[_]SBAR[+]S[+]VP', '15[_]SBAR[_]WHNP', '15[_]S[+]VP', '15[_]S[+]VP[_]ADVP', '15[_]VP', '16[_]ADJP', '16[_]NP', '16[_]NP[+]QP', '16[_]NP[_]NP', '16[_]PP', '16[_]S', '16[_]S[+]VP', '16[_]S[+]VP[_]ADVP', '16[_]S[+]VP[_]NP', '16[_]S[_]NP', '16[_]VP', '16[_]VP[_]NP', '17[_]NP', '17[_]PP', '17[_]SBAR[_]WHADVP', '17[_]SQ', '17[_]SQ[_]NP', '17[_]SQ[_]VP', '17[_]S[+]VP', '17[_]S[_]NP', '17[_]VP', '18[_]NP', '18[_]PP', '18[_]S', '18[_]VP', '18[_]VP[_]PRT', '19[_]ADJP', '19[_]NP', '19[_]PP', '1[_]FRAG', '1[_]NP', '1[_]S', '1[_]SBARQ', '1[_]SBARQ[_]ADJP', '1[_]SBARQ[_]WHNP', '1[_]SINV', '1[_]SINV[_]ADVP', '1[_]SINV[_]NP', '1[_]SINV[_]PRT', '1[_]SINV[_]VP', '1[_]SQ', '1[_]SQ[_]NP', '1[_]SQ[_]VP', '1[_]S[_]ADJP', '1[_]S[_]ADVP', '1[_]S[_]INTJ', '1[_]S[_]NP', '1[_]S[_]NP[+]NP', '1[_]S[_]S[+]VP', '1[_]S[_]VP', '1[_]UCP', '20[_]NP', '20[_]PP', '20[_]QP', '21[_]NP', '2[_]ADVP', '2[_]ADVP[_]NP', '2[_]NP', '2[_]NP[_]NP', '2[_]NP[_]NX', '2[_]PP', '2[_]PRN', '2[_]PRN[_]VP', '2[_]S', '2[_]SBAR', '2[_]SBAR[_]WHADVP', '2[_]SBAR[_]WHNP', '2[_]SINV', '2[_]SINV[_]VP', '2[_]SQ', '2[_]SQ[_]ADVP', '2[_]SQ[_]NP', '2[_]S[_]ADJP', '2[_]S[_]ADVP', '2[_]S[_]NP', '2[_]S[_]NP[+]NP', '2[_]S[_]PRT', '2[_]S[_]S[+]VP', '2[_]S[_]VP', '2[_]VP', '2[_]VP[_]ADJP', '2[_]VP[_]ADVP', '2[_]VP[_]NP', '2[_]VP[_]PRT', '2[_]VP[_]S[+]VP', '2[_]VP[_]VP', '3[_]ADJP', '3[_]ADVP', '3[_]NAC', '3[_]NP', '3[_]NP[+]NP', '3[_]NP[+]QP', '3[_]NP[_]ADVP', '3[_]NP[_]NP', '3[_]NP[_]PP', '3[_]NX', '3[_]PP', '3[_]PRN', '3[_]PRN[_]VP', '3[_]QP', '3[_]S', '3[_]SBAR', '3[_]SBAR[_]VP', '3[_]SBAR[_]WHADVP', '3[_]SBAR[_]WHNP', '3[_]SQ[+]VP', '3[_]S[+]ADJP', '3[_]S[+]VP', '3[_]S[_]ADVP', '3[_]S[_]NP', '3[_]S[_]NP[+]NP', '3[_]UCP', '3[_]VP', '3[_]VP[_]ADJP', '3[_]VP[_]ADVP', '3[_]VP[_]ADVP[+]ADVP', '3[_]VP[_]NP', '3[_]VP[_]PRT', '3[_]VP[_]VP', '4[_]ADJP', '4[_]ADJP[+]QP', '4[_]ADJP[_]VP', '4[_]ADVP', '4[_]NP', '4[_]NP[+]NP', '4[_]NP[_]NP', '4[_]NP[_]VP', '4[_]NX', '4[_]PP', '4[_]PP[_]ADVP', '4[_]PRN', '4[_]PRN[_]NP', '4[_]QP', '4[_]S', '4[_]SBAR', '4[_]SBAR[+]S', '4[_]SBAR[+]S[_]NP', '4[_]SBAR[+]S[_]NP[+]NP', '4[_]SBAR[_]NP', '4[_]SBAR[_]WHADVP', '4[_]SBAR[_]WHNP', '4[_]S[+]ADJP', '4[_]S[+]VP', '4[_]S[+]VP[_]ADVP', '4[_]S[+]VP[_]NP', '4[_]S[+]VP[_]PRT', '4[_]S[_]ADJP', '4[_]S[_]ADVP', '4[_]S[_]NP', '4[_]S[_]S[+]VP', '4[_]UCP', '4[_]VP', '4[_]VP[+]VP', '4[_]VP[+]VP[_]PRT', '4[_]VP[_]NP', '4[_]VP[_]PP', '4[_]VP[_]PRT', '4[_]VP[_]VP', '4[_]WHNP', '4[_]WHPP', '5[_]ADJP', '5[_]ADVP', '5[_]NP', '5[_]NP[+]NP', '5[_]NP[_]ADJP', '5[_]NP[_]NP', '5[_]NP[_]PP', '5[_]NP[_]VP', '5[_]PP', '5[_]PRN', '5[_]PRN[_]VP', '5[_]QP', '5[_]S', '5[_]SBAR', '5[_]SBAR[+]S', '5[_]SBAR[+]S[+]VP', '5[_]SBAR[+]S[_]NP', '5[_]SBAR[_]WHADVP', '5[_]SBAR[_]WHNP', '5[_]S[+]NP', '5[_]S[+]VP', '5[_]S[+]VP[_]ADVP', '5[_]S[+]VP[_]NP', '5[_]S[_]ADVP', '5[_]S[_]NP', '5[_]VP', '5[_]VP[_]ADJP', '5[_]VP[_]ADVP', '5[_]VP[_]NP', '5[_]VP[_]PRT', '5[_]WHNP', '5[_]WHPP', '6[_]ADJP', '6[_]ADVP', '6[_]ADVP[_]NP', '6[_]INTJ', '6[_]LST', '6[_]NP', '6[_]NP[+]NP', '6[_]NP[+]QP', '6[_]NP[_]NP', '6[_]NX', '6[_]PP', '6[_]PP[_]ADVP', '6[_]PRN', '6[_]QP', '6[_]S', '6[_]SBAR', '6[_]SBAR[+]S', '6[_]SBAR[+]S[+]VP', '6[_]SBAR[+]S[_]NP', '6[_]SBAR[+]S[_]VP', '6[_]SBAR[_]VP', '6[_]SBAR[_]WHADVP', '6[_]SBAR[_]WHNP', '6[_]S[+]VP', '6[_]S[_]ADVP', '6[_]S[_]NP', '6[_]S[_]VP', '6[_]VP', '6[_]VP[_]ADVP', '6[_]VP[_]NP', '6[_]VP[_]PP', '6[_]VP[_]PRT', '7[_]ADJP', '7[_]ADJP[_]ADJP', '7[_]ADVP', '7[_]ADVP[_]ADVP', '7[_]FRAG', '7[_]NP', '7[_]NP[+]NP', '7[_]NP[+]QP', '7[_]NP[_]NP', '7[_]NX', '7[_]NX[_]NP', '7[_]PP', '7[_]PRN', '7[_]PRN[_]NP', '7[_]QP', '7[_]S', '7[_]SBAR', '7[_]SBAR[+]S', '7[_]SBAR[+]SINV', '7[_]SBAR[+]SINV[_]NP', '7[_]SBAR[+]S[+]VP', '7[_]SBAR[+]S[_]NP', '7[_]SBAR[_]WHADVP', '7[_]SBAR[_]WHNP', '7[_]S[+]VP', '7[_]S[+]VP[_]NP', '7[_]S[_]ADVP', '7[_]S[_]NP', '7[_]VP', '7[_]VP[_]ADVP', '7[_]VP[_]NP', '7[_]VP[_]VP', '7[_]WHPP', '8[_]ADJP', '8[_]ADVP', '8[_]NAC', '8[_]NP', '8[_]NP[+]QP', '8[_]NP[_]NP', '8[_]NX', '8[_]PP', '8[_]PRN', '8[_]QP', '8[_]S', '8[_]SBAR', '8[_]SBAR[+]S', '8[_]SBAR[+]S[+]VP', '8[_]SBAR[_]WHADVP', '8[_]SBAR[_]WHNP', '8[_]S[+]VP', '8[_]S[+]VP[_]ADVP', '8[_]S[_]ADVP', '8[_]S[_]NP', '8[_]S[_]NP[+]NP', '8[_]S[_]S[+]VP', '8[_]UCP', '8[_]VP', '8[_]VP[_]ADJP', '8[_]VP[_]ADVP', '8[_]VP[_]NP', '8[_]VP[_]PRT', '8[_]VP[_]VP', '8[_]WHPP', '9[_]ADJP', '9[_]ADJP[+]QP', '9[_]NAC', '9[_]NP', '9[_]NP[+]QP', '9[_]NP[_]NP', '9[_]PP', '9[_]PRN', '9[_]PRN[_]VP', '9[_]QP', '9[_]S', '9[_]SBAR', '9[_]SBAR[+]S', '9[_]SBAR[_]ADVP', '9[_]SBAR[_]WHNP', '9[_]S[+]PP', '9[_]S[+]VP', '9[_]S[+]VP[_]NP', '9[_]S[+]VP[_]VP', '9[_]S[_]NP', '9[_]VP', '9[_]VP[_]ADJP', '9[_]VP[_]ADVP', '9[_]VP[_]NP', '9[_]VP[_]PRT', '9[_]WHPP']\n",
      "[*] Loaded TokenClassification task with 331 labels: ['-1*[_]ADJP', '-1*[_]ADVP', '-1*[_]FRAG', '-1*[_]NP', '-1*[_]NP[+]NP', '-1*[_]NP[_]ADJP', '-1*[_]NP[_]ADVP', '-1*[_]NP[_]NP', '-1*[_]NP[_]NX', '-1*[_]NX', '-1*[_]PP', '-1*[_]PRN', '-1*[_]PRN[_]VP', '-1*[_]S', '-1*[_]SBAR', '-1*[_]SBAR[+]S', '-1*[_]SBAR[_]WHNP', '-1*[_]SINV', '-1*[_]SINV[_]VP', '-1*[_]S[+]ADJP', '-1*[_]S[+]VP', '-1*[_]S[+]VP[_]NP', '-1*[_]S[+]VP[_]VP', '-1*[_]S[_]ADJP', '-1*[_]S[_]ADVP', '-1*[_]S[_]NP', '-1*[_]S[_]S[+]VP', '-1*[_]S[_]VP', '-1*[_]UCP', '-1*[_]VP', '-1*[_]VP[_]ADVP', '-1*[_]VP[_]NP', '-1*[_]VP[_]S[+]VP', '-1*[_]VP[_]VP', '-10*[_]S', '-10*[_]SINV', '-10*[_]S[_]ADVP', '-10*[_]S[_]NP', '-10*[_]S[_]VP', '-10*[_]UCP', '-11*[_]S', '-11*[_]SINV', '-11*[_]S[_]ADJP', '-11*[_]S[_]ADVP', '-11*[_]S[_]NP', '-11*[_]S[_]VP', '-11*[_]VP', '-12*[_]S', '-12*[_]S[_]ADVP', '-12*[_]S[_]NP', '-12*[_]S[_]VP', '-13*[_]S', '-13*[_]SINV', '-13*[_]S[_]NP', '-14*[_]S', '-14*[_]SINV[_]ADVP', '-14*[_]S[_]ADJP', '-14*[_]S[_]NP', '-14*[_]S[_]VP', '-15*[_]S', '-15*[_]S[_]NP', '-16*[_]S', '-16*[_]S[_]NP', '-17*[_]S', '-17*[_]SINV', '-18*[_]S', '-19*[_]S[_]NP', '-2*[_]ADJP', '-2*[_]ADVP', '-2*[_]ADVP[_]NP', '-2*[_]NP', '-2*[_]NP[_]NP', '-2*[_]NP[_]VP', '-2*[_]PP', '-2*[_]PRN', '-2*[_]PRN[_]VP', '-2*[_]S', '-2*[_]SBAR', '-2*[_]SBARQ[_]ADJP', '-2*[_]SBAR[+]S', '-2*[_]SBAR[+]S[_]NP', '-2*[_]SINV', '-2*[_]SINV[_]PRT', '-2*[_]S[+]VP', '-2*[_]S[_]ADJP', '-2*[_]S[_]ADVP', '-2*[_]S[_]NP', '-2*[_]S[_]S[+]VP', '-2*[_]S[_]VP', '-2*[_]VP', '-2*[_]VP[_]ADVP', '-2*[_]VP[_]NP', '-2*[_]VP[_]PRT', '-3*[_]NP', '-3*[_]NP[_]NP', '-3*[_]PRN', '-3*[_]PRN[_]VP', '-3*[_]S', '-3*[_]SBAR[+]S', '-3*[_]SBAR[_]VP', '-3*[_]SINV', '-3*[_]SINV[_]NP', '-3*[_]SQ', '-3*[_]SQ[_]VP', '-3*[_]S[+]NP', '-3*[_]S[+]VP', '-3*[_]S[_]ADJP', '-3*[_]S[_]ADVP', '-3*[_]S[_]NP', '-3*[_]S[_]PRT', '-3*[_]VP', '-3*[_]VP[_]ADJP', '-3*[_]VP[_]NP', '-3*[_]VP[_]VP', '-4*[_]ADJP[_]VP', '-4*[_]NP', '-4*[_]NP[_]NP', '-4*[_]PRN', '-4*[_]S', '-4*[_]SBAR[+]S[_]VP', '-4*[_]SBAR[_]VP', '-4*[_]SINV', '-4*[_]S[_]ADJP', '-4*[_]S[_]ADVP', '-4*[_]S[_]NP', '-4*[_]S[_]S[+]VP', '-4*[_]S[_]VP', '-4*[_]UCP', '-4*[_]VP', '-4*[_]VP[_]ADVP', '-4*[_]VP[_]NP', '-5*[_]NP', '-5*[_]NP[_]NP', '-5*[_]NP[_]PP', '-5*[_]S', '-5*[_]SBAR', '-5*[_]SINV', '-5*[_]SINV[_]ADVP', '-5*[_]SQ', '-5*[_]S[+]VP', '-5*[_]S[_]ADVP', '-5*[_]S[_]NP', '-5*[_]S[_]VP', '-5*[_]VP', '-5*[_]VP[_]NP', '-5*[_]VP[_]PP', '-6*[_]FRAG', '-6*[_]NP', '-6*[_]PRN[_]VP', '-6*[_]S', '-6*[_]SBAR[+]S', '-6*[_]SBAR[_]NP', '-6*[_]SINV', '-6*[_]S[+]VP[_]VP', '-6*[_]S[_]ADVP', '-6*[_]S[_]NP', '-6*[_]S[_]PRT', '-6*[_]S[_]VP', '-6*[_]VP', '-6*[_]VP[_]VP', '-7*[_]NP', '-7*[_]NP[_]PP', '-7*[_]S', '-7*[_]SINV', '-7*[_]SQ[_]NP', '-7*[_]S[_]NP', '-7*[_]S[_]VP', '-7*[_]VP', '-8*[_]NP', '-8*[_]NP[_]VP', '-8*[_]S', '-8*[_]SBAR', '-8*[_]SINV', '-8*[_]SINV[_]NP', '-8*[_]S[_]ADVP', '-8*[_]S[_]NP', '-8*[_]S[_]VP', '-8*[_]VP', '-9*[_]FRAG', '-9*[_]NP', '-9*[_]NP[_]NP', '-9*[_]S', '-9*[_]SINV[_]VP', '-9*[_]S[_]ADVP', '-9*[_]S[_]NP', '-9*[_]S[_]VP', '0*[_]ADJP', '0*[_]ADJP[+]QP', '0*[_]ADVP', '0*[_]FRAG', '0*[_]NAC', '0*[_]NP', '0*[_]NP[+]NP', '0*[_]NP[+]QP', '0*[_]NP[_]ADJP', '0*[_]NP[_]ADVP', '0*[_]NP[_]NP', '0*[_]NP[_]NX', '0*[_]NX', '0*[_]PP', '0*[_]PP[_]INTJ', '0*[_]PP[_]NP', '0*[_]PRN', '0*[_]PRN[_]NP', '0*[_]QP', '0*[_]S', '0*[_]SBAR', '0*[_]SBARQ', '0*[_]SBAR[+]S', '0*[_]SBAR[+]SINV', '0*[_]SBAR[+]SINV[_]NP', '0*[_]SBAR[+]S[_]NP[+]NP', '0*[_]SINV', '0*[_]SINV[_]VP', '0*[_]SQ', '0*[_]SQ[_]ADVP', '0*[_]SQ[_]NP', '0*[_]SQ[_]VP', '0*[_]S[+]NP', '0*[_]S[+]VP', '0*[_]S[+]VP[_]ADVP', '0*[_]S[+]VP[_]NP', '0*[_]S[+]VP[_]PRT', '0*[_]S[_]ADVP', '0*[_]S[_]NP', '0*[_]S[_]NP[+]NP', '0*[_]S[_]VP', '0*[_]UCP', '0*[_]VP', '0*[_]VP[+]VP[_]PRT', '0*[_]VP[_]ADJP', '0*[_]VP[_]ADVP', '0*[_]VP[_]ADVP[+]ADVP', '0*[_]VP[_]NP', '0*[_]VP[_]PP', '0*[_]VP[_]PRT', '0*[_]VP[_]VP', '1*[_]ADJP', '1*[_]ADJP[_]ADJP', '1*[_]ADVP', '1*[_]ADVP[_]ADVP', '1*[_]CONJP', '1*[_]FRAG', '1*[_]INTJ', '1*[_]NAC', '1*[_]NP', '1*[_]NP[_]ADVP', '1*[_]NP[_]NP', '1*[_]NX', '1*[_]NX[_]NP', '1*[_]PP', '1*[_]PP[_]ADVP', '1*[_]PRN', '1*[_]QP', '1*[_]S', '1*[_]SBAR', '1*[_]SBARQ[_]WHNP', '1*[_]SBAR[_]ADVP', '1*[_]SBAR[_]WHADVP', '1*[_]SBAR[_]WHNP', '1*[_]SINV', '1*[_]SINV[_]ADVP', '1*[_]SINV[_]VP', '1*[_]SQ', '1*[_]S[_]ADVP', '1*[_]S[_]INTJ', '1*[_]S[_]NP', '1*[_]S[_]NP[+]NP', '1*[_]VP', '1*[_]VP[_]ADVP', '1*[_]VP[_]VP', '1*[_]WHNP', '2*[_]ADJP', '2*[_]ADJP[+]QP', '2*[_]ADVP', '2*[_]ADVP[_]NP', '2*[_]LST', '2*[_]NP', '2*[_]NP[+]NP', '2*[_]NP[+]QP', '2*[_]NP[_]NP', '2*[_]NX', '2*[_]PP', '2*[_]PP[_]ADVP', '2*[_]PRN', '2*[_]QP', '2*[_]S', '2*[_]SBAR', '2*[_]SBAR[+]S', '2*[_]SBAR[+]SINV', '2*[_]SBAR[+]S[_]NP', '2*[_]SBAR[+]S[_]NP[+]NP', '2*[_]SBAR[_]WHADVP', '2*[_]SBAR[_]WHNP', '2*[_]SQ[+]VP', '2*[_]S[+]ADJP', '2*[_]S[+]NP[_]NP', '2*[_]S[+]PP', '2*[_]S[+]VP', '2*[_]S[+]VP[_]ADVP', '2*[_]S[_]ADJP', '2*[_]S[_]ADVP', '2*[_]S[_]NP', '2*[_]S[_]NP[+]NP', '2*[_]UCP', '2*[_]VP', '2*[_]VP[+]VP', '2*[_]VP[_]VP', '2*[_]WHNP', '2*[_]WHPP', '3*[_]ADJP', '3*[_]ADVP', '3*[_]LST', '3*[_]NAC', '3*[_]NP', '3*[_]NP[+]NP', '3*[_]NP[+]QP', '3*[_]NP[_]NP', '3*[_]PP', '3*[_]QP', '3*[_]S', '3*[_]SBAR', '3*[_]SBAR[+]S[+]VP', '3*[_]S[+]VP', '3*[_]VP', '3*[_]VP[_]ADVP', '4*[_]ADJP', '4*[_]NP', '4*[_]NP[+]NP', '4*[_]NP[_]NP', '4*[_]QP']\n",
      "[*] Found task 0 => encoding_abs with 410 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Found task 1 => encoding_rel with 331 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Preprocessing task 0 => encoding_abs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Preprocessing task 1 => encoding_rel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]              You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 8/8 [00:02<00:00,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Evaluating task 0 => encoding_abs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 8/8 [00:02<00:00,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.022686004638672, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.0006236357966947302, 'eval_name': 'encoding_abs', 'eval_size': 100, 'eval_index': 0, 'eval_runtime': 0.4531, 'eval_samples_per_second': 220.712, 'eval_steps_per_second': 8.828, 'epoch': 1.0}\n",
      "[*] Evaluating task 1 => encoding_rel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 8/8 [00:03<00:00,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.805825233459473, 'eval_precision': 0.0038910505836575876, 'eval_recall': 0.0010090817356205853, 'eval_f1': 0.0016025641025641025, 'eval_accuracy': 0.0003118178983473651, 'eval_name': 'encoding_rel', 'eval_size': 100, 'eval_index': 1, 'eval_runtime': 0.4478, 'eval_samples_per_second': 223.308, 'eval_steps_per_second': 8.932, 'epoch': 1.0}\n",
      "[*] Calling save_model with task_idx 0\n",
      "[*] Initializing adapter with 2 classifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['Z', 'shared_encoder.embeddings.position_ids', 'shared_encoder.embeddings.word_embeddings.weight', 'shared_encoder.embeddings.position_embeddings.weight', 'shared_encoder.embeddings.token_type_embeddings.weight', 'shared_encoder.embeddings.LayerNorm.weight', 'shared_encoder.embeddings.LayerNorm.bias', 'shared_encoder.encoder.layer.0.attention.self.query.weight', 'shared_encoder.encoder.layer.0.attention.self.query.bias', 'shared_encoder.encoder.layer.0.attention.self.key.weight', 'shared_encoder.encoder.layer.0.attention.self.key.bias', 'shared_encoder.encoder.layer.0.attention.self.value.weight', 'shared_encoder.encoder.layer.0.attention.self.value.bias', 'shared_encoder.encoder.layer.0.attention.output.dense.weight', 'shared_encoder.encoder.layer.0.attention.output.dense.bias', 'shared_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.0.intermediate.dense.weight', 'shared_encoder.encoder.layer.0.intermediate.dense.bias', 'shared_encoder.encoder.layer.0.output.dense.weight', 'shared_encoder.encoder.layer.0.output.dense.bias', 'shared_encoder.encoder.layer.0.output.LayerNorm.weight', 'shared_encoder.encoder.layer.0.output.LayerNorm.bias', 'shared_encoder.encoder.layer.1.attention.self.query.weight', 'shared_encoder.encoder.layer.1.attention.self.query.bias', 'shared_encoder.encoder.layer.1.attention.self.key.weight', 'shared_encoder.encoder.layer.1.attention.self.key.bias', 'shared_encoder.encoder.layer.1.attention.self.value.weight', 'shared_encoder.encoder.layer.1.attention.self.value.bias', 'shared_encoder.encoder.layer.1.attention.output.dense.weight', 'shared_encoder.encoder.layer.1.attention.output.dense.bias', 'shared_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.1.intermediate.dense.weight', 'shared_encoder.encoder.layer.1.intermediate.dense.bias', 'shared_encoder.encoder.layer.1.output.dense.weight', 'shared_encoder.encoder.layer.1.output.dense.bias', 'shared_encoder.encoder.layer.1.output.LayerNorm.weight', 'shared_encoder.encoder.layer.1.output.LayerNorm.bias', 'shared_encoder.encoder.layer.2.attention.self.query.weight', 'shared_encoder.encoder.layer.2.attention.self.query.bias', 'shared_encoder.encoder.layer.2.attention.self.key.weight', 'shared_encoder.encoder.layer.2.attention.self.key.bias', 'shared_encoder.encoder.layer.2.attention.self.value.weight', 'shared_encoder.encoder.layer.2.attention.self.value.bias', 'shared_encoder.encoder.layer.2.attention.output.dense.weight', 'shared_encoder.encoder.layer.2.attention.output.dense.bias', 'shared_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.2.intermediate.dense.weight', 'shared_encoder.encoder.layer.2.intermediate.dense.bias', 'shared_encoder.encoder.layer.2.output.dense.weight', 'shared_encoder.encoder.layer.2.output.dense.bias', 'shared_encoder.encoder.layer.2.output.LayerNorm.weight', 'shared_encoder.encoder.layer.2.output.LayerNorm.bias', 'shared_encoder.encoder.layer.3.attention.self.query.weight', 'shared_encoder.encoder.layer.3.attention.self.query.bias', 'shared_encoder.encoder.layer.3.attention.self.key.weight', 'shared_encoder.encoder.layer.3.attention.self.key.bias', 'shared_encoder.encoder.layer.3.attention.self.value.weight', 'shared_encoder.encoder.layer.3.attention.self.value.bias', 'shared_encoder.encoder.layer.3.attention.output.dense.weight', 'shared_encoder.encoder.layer.3.attention.output.dense.bias', 'shared_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.3.intermediate.dense.weight', 'shared_encoder.encoder.layer.3.intermediate.dense.bias', 'shared_encoder.encoder.layer.3.output.dense.weight', 'shared_encoder.encoder.layer.3.output.dense.bias', 'shared_encoder.encoder.layer.3.output.LayerNorm.weight', 'shared_encoder.encoder.layer.3.output.LayerNorm.bias', 'shared_encoder.encoder.layer.4.attention.self.query.weight', 'shared_encoder.encoder.layer.4.attention.self.query.bias', 'shared_encoder.encoder.layer.4.attention.self.key.weight', 'shared_encoder.encoder.layer.4.attention.self.key.bias', 'shared_encoder.encoder.layer.4.attention.self.value.weight', 'shared_encoder.encoder.layer.4.attention.self.value.bias', 'shared_encoder.encoder.layer.4.attention.output.dense.weight', 'shared_encoder.encoder.layer.4.attention.output.dense.bias', 'shared_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.4.intermediate.dense.weight', 'shared_encoder.encoder.layer.4.intermediate.dense.bias', 'shared_encoder.encoder.layer.4.output.dense.weight', 'shared_encoder.encoder.layer.4.output.dense.bias', 'shared_encoder.encoder.layer.4.output.LayerNorm.weight', 'shared_encoder.encoder.layer.4.output.LayerNorm.bias', 'shared_encoder.encoder.layer.5.attention.self.query.weight', 'shared_encoder.encoder.layer.5.attention.self.query.bias', 'shared_encoder.encoder.layer.5.attention.self.key.weight', 'shared_encoder.encoder.layer.5.attention.self.key.bias', 'shared_encoder.encoder.layer.5.attention.self.value.weight', 'shared_encoder.encoder.layer.5.attention.self.value.bias', 'shared_encoder.encoder.layer.5.attention.output.dense.weight', 'shared_encoder.encoder.layer.5.attention.output.dense.bias', 'shared_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.5.intermediate.dense.weight', 'shared_encoder.encoder.layer.5.intermediate.dense.bias', 'shared_encoder.encoder.layer.5.output.dense.weight', 'shared_encoder.encoder.layer.5.output.dense.bias', 'shared_encoder.encoder.layer.5.output.LayerNorm.weight', 'shared_encoder.encoder.layer.5.output.LayerNorm.bias', 'shared_encoder.encoder.layer.6.attention.self.query.weight', 'shared_encoder.encoder.layer.6.attention.self.query.bias', 'shared_encoder.encoder.layer.6.attention.self.key.weight', 'shared_encoder.encoder.layer.6.attention.self.key.bias', 'shared_encoder.encoder.layer.6.attention.self.value.weight', 'shared_encoder.encoder.layer.6.attention.self.value.bias', 'shared_encoder.encoder.layer.6.attention.output.dense.weight', 'shared_encoder.encoder.layer.6.attention.output.dense.bias', 'shared_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.6.intermediate.dense.weight', 'shared_encoder.encoder.layer.6.intermediate.dense.bias', 'shared_encoder.encoder.layer.6.output.dense.weight', 'shared_encoder.encoder.layer.6.output.dense.bias', 'shared_encoder.encoder.layer.6.output.LayerNorm.weight', 'shared_encoder.encoder.layer.6.output.LayerNorm.bias', 'shared_encoder.encoder.layer.7.attention.self.query.weight', 'shared_encoder.encoder.layer.7.attention.self.query.bias', 'shared_encoder.encoder.layer.7.attention.self.key.weight', 'shared_encoder.encoder.layer.7.attention.self.key.bias', 'shared_encoder.encoder.layer.7.attention.self.value.weight', 'shared_encoder.encoder.layer.7.attention.self.value.bias', 'shared_encoder.encoder.layer.7.attention.output.dense.weight', 'shared_encoder.encoder.layer.7.attention.output.dense.bias', 'shared_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.7.intermediate.dense.weight', 'shared_encoder.encoder.layer.7.intermediate.dense.bias', 'shared_encoder.encoder.layer.7.output.dense.weight', 'shared_encoder.encoder.layer.7.output.dense.bias', 'shared_encoder.encoder.layer.7.output.LayerNorm.weight', 'shared_encoder.encoder.layer.7.output.LayerNorm.bias', 'shared_encoder.encoder.layer.8.attention.self.query.weight', 'shared_encoder.encoder.layer.8.attention.self.query.bias', 'shared_encoder.encoder.layer.8.attention.self.key.weight', 'shared_encoder.encoder.layer.8.attention.self.key.bias', 'shared_encoder.encoder.layer.8.attention.self.value.weight', 'shared_encoder.encoder.layer.8.attention.self.value.bias', 'shared_encoder.encoder.layer.8.attention.output.dense.weight', 'shared_encoder.encoder.layer.8.attention.output.dense.bias', 'shared_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.8.intermediate.dense.weight', 'shared_encoder.encoder.layer.8.intermediate.dense.bias', 'shared_encoder.encoder.layer.8.output.dense.weight', 'shared_encoder.encoder.layer.8.output.dense.bias', 'shared_encoder.encoder.layer.8.output.LayerNorm.weight', 'shared_encoder.encoder.layer.8.output.LayerNorm.bias', 'shared_encoder.encoder.layer.9.attention.self.query.weight', 'shared_encoder.encoder.layer.9.attention.self.query.bias', 'shared_encoder.encoder.layer.9.attention.self.key.weight', 'shared_encoder.encoder.layer.9.attention.self.key.bias', 'shared_encoder.encoder.layer.9.attention.self.value.weight', 'shared_encoder.encoder.layer.9.attention.self.value.bias', 'shared_encoder.encoder.layer.9.attention.output.dense.weight', 'shared_encoder.encoder.layer.9.attention.output.dense.bias', 'shared_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.9.intermediate.dense.weight', 'shared_encoder.encoder.layer.9.intermediate.dense.bias', 'shared_encoder.encoder.layer.9.output.dense.weight', 'shared_encoder.encoder.layer.9.output.dense.bias', 'shared_encoder.encoder.layer.9.output.LayerNorm.weight', 'shared_encoder.encoder.layer.9.output.LayerNorm.bias', 'shared_encoder.encoder.layer.10.attention.self.query.weight', 'shared_encoder.encoder.layer.10.attention.self.query.bias', 'shared_encoder.encoder.layer.10.attention.self.key.weight', 'shared_encoder.encoder.layer.10.attention.self.key.bias', 'shared_encoder.encoder.layer.10.attention.self.value.weight', 'shared_encoder.encoder.layer.10.attention.self.value.bias', 'shared_encoder.encoder.layer.10.attention.output.dense.weight', 'shared_encoder.encoder.layer.10.attention.output.dense.bias', 'shared_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.10.intermediate.dense.weight', 'shared_encoder.encoder.layer.10.intermediate.dense.bias', 'shared_encoder.encoder.layer.10.output.dense.weight', 'shared_encoder.encoder.layer.10.output.dense.bias', 'shared_encoder.encoder.layer.10.output.LayerNorm.weight', 'shared_encoder.encoder.layer.10.output.LayerNorm.bias', 'shared_encoder.encoder.layer.11.attention.self.query.weight', 'shared_encoder.encoder.layer.11.attention.self.query.bias', 'shared_encoder.encoder.layer.11.attention.self.key.weight', 'shared_encoder.encoder.layer.11.attention.self.key.bias', 'shared_encoder.encoder.layer.11.attention.self.value.weight', 'shared_encoder.encoder.layer.11.attention.self.value.bias', 'shared_encoder.encoder.layer.11.attention.output.dense.weight', 'shared_encoder.encoder.layer.11.attention.output.dense.bias', 'shared_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'shared_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'shared_encoder.encoder.layer.11.intermediate.dense.weight', 'shared_encoder.encoder.layer.11.intermediate.dense.bias', 'shared_encoder.encoder.layer.11.output.dense.weight', 'shared_encoder.encoder.layer.11.output.dense.bias', 'shared_encoder.encoder.layer.11.output.LayerNorm.weight', 'shared_encoder.encoder.layer.11.output.LayerNorm.bias', 'task_models_list.0.roberta.embeddings.position_ids', 'task_models_list.0.roberta.embeddings.word_embeddings.weight', 'task_models_list.0.roberta.embeddings.position_embeddings.weight', 'task_models_list.0.roberta.embeddings.token_type_embeddings.weight', 'task_models_list.0.roberta.embeddings.LayerNorm.weight', 'task_models_list.0.roberta.embeddings.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.0.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.0.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.0.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.0.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.0.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.0.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.0.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.0.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.0.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.0.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.0.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.0.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.0.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.0.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.1.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.1.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.1.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.1.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.1.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.1.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.1.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.1.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.1.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.1.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.1.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.1.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.1.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.1.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.2.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.2.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.2.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.2.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.2.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.2.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.2.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.2.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.2.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.2.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.2.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.2.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.2.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.2.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.3.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.3.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.3.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.3.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.3.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.3.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.3.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.3.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.3.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.3.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.3.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.3.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.3.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.3.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.4.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.4.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.4.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.4.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.4.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.4.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.4.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.4.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.4.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.4.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.4.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.4.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.4.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.4.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.5.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.5.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.5.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.5.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.5.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.5.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.5.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.5.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.5.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.5.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.5.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.5.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.5.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.5.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.6.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.6.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.6.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.6.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.6.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.6.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.6.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.6.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.6.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.6.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.6.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.6.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.6.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.6.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.7.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.7.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.7.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.7.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.7.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.7.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.7.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.7.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.7.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.7.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.7.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.7.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.7.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.7.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.8.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.8.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.8.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.8.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.8.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.8.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.8.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.8.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.8.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.8.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.8.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.8.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.8.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.8.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.9.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.9.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.9.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.9.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.9.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.9.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.9.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.9.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.9.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.9.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.9.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.9.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.9.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.9.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.10.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.10.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.10.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.10.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.10.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.10.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.10.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.10.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.10.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.10.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.10.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.10.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.10.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.10.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.11.attention.self.query.weight', 'task_models_list.0.roberta.encoder.layer.11.attention.self.query.bias', 'task_models_list.0.roberta.encoder.layer.11.attention.self.key.weight', 'task_models_list.0.roberta.encoder.layer.11.attention.self.key.bias', 'task_models_list.0.roberta.encoder.layer.11.attention.self.value.weight', 'task_models_list.0.roberta.encoder.layer.11.attention.self.value.bias', 'task_models_list.0.roberta.encoder.layer.11.attention.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.11.attention.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'task_models_list.0.roberta.encoder.layer.11.intermediate.dense.weight', 'task_models_list.0.roberta.encoder.layer.11.intermediate.dense.bias', 'task_models_list.0.roberta.encoder.layer.11.output.dense.weight', 'task_models_list.0.roberta.encoder.layer.11.output.dense.bias', 'task_models_list.0.roberta.encoder.layer.11.output.LayerNorm.weight', 'task_models_list.0.roberta.encoder.layer.11.output.LayerNorm.bias', 'task_models_list.0.classifier.weight', 'task_models_list.0.classifier.bias', 'task_models_list.1.roberta.embeddings.position_ids', 'task_models_list.1.roberta.embeddings.word_embeddings.0.weight', 'task_models_list.1.roberta.embeddings.position_embeddings.weight', 'task_models_list.1.roberta.embeddings.token_type_embeddings.weight', 'task_models_list.1.roberta.embeddings.LayerNorm.weight', 'task_models_list.1.roberta.embeddings.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.0.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.0.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.0.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.0.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.0.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.0.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.0.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.0.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.0.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.0.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.0.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.0.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.0.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.0.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.1.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.1.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.1.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.1.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.1.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.1.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.1.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.1.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.1.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.1.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.1.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.1.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.1.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.1.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.2.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.2.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.2.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.2.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.2.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.2.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.2.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.2.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.2.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.2.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.2.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.2.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.2.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.2.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.3.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.3.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.3.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.3.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.3.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.3.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.3.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.3.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.3.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.3.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.3.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.3.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.3.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.3.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.4.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.4.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.4.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.4.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.4.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.4.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.4.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.4.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.4.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.4.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.4.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.4.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.4.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.4.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.5.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.5.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.5.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.5.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.5.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.5.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.5.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.5.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.5.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.5.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.5.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.5.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.5.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.5.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.6.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.6.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.6.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.6.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.6.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.6.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.6.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.6.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.6.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.6.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.6.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.6.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.6.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.6.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.7.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.7.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.7.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.7.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.7.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.7.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.7.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.7.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.7.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.7.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.7.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.7.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.7.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.7.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.8.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.8.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.8.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.8.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.8.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.8.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.8.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.8.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.8.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.8.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.8.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.8.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.8.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.8.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.9.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.9.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.9.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.9.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.9.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.9.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.9.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.9.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.9.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.9.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.9.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.9.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.9.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.9.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.10.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.10.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.10.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.10.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.10.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.10.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.10.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.10.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.10.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.10.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.10.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.10.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.10.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.10.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.11.attention.self.query.weight', 'task_models_list.1.roberta.encoder.layer.11.attention.self.query.bias', 'task_models_list.1.roberta.encoder.layer.11.attention.self.key.weight', 'task_models_list.1.roberta.encoder.layer.11.attention.self.key.bias', 'task_models_list.1.roberta.encoder.layer.11.attention.self.value.weight', 'task_models_list.1.roberta.encoder.layer.11.attention.self.value.bias', 'task_models_list.1.roberta.encoder.layer.11.attention.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.11.attention.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'task_models_list.1.roberta.encoder.layer.11.intermediate.dense.weight', 'task_models_list.1.roberta.encoder.layer.11.intermediate.dense.bias', 'task_models_list.1.roberta.encoder.layer.11.output.dense.weight', 'task_models_list.1.roberta.encoder.layer.11.output.dense.bias', 'task_models_list.1.roberta.encoder.layer.11.output.LayerNorm.weight', 'task_models_list.1.roberta.encoder.layer.11.output.LayerNorm.bias', 'task_models_list.1.classifier.weight', 'task_models_list.1.classifier.bias', 'task_models_list.1.auto.embeddings.position_ids', 'task_models_list.1.auto.embeddings.word_embeddings.0.weight', 'task_models_list.1.auto.embeddings.position_embeddings.weight', 'task_models_list.1.auto.embeddings.token_type_embeddings.weight', 'task_models_list.1.auto.embeddings.LayerNorm.weight', 'task_models_list.1.auto.embeddings.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.0.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.0.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.0.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.0.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.0.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.0.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.0.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.0.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.0.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.0.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.0.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.0.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.0.output.dense.weight', 'task_models_list.1.auto.encoder.layer.0.output.dense.bias', 'task_models_list.1.auto.encoder.layer.0.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.0.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.1.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.1.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.1.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.1.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.1.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.1.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.1.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.1.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.1.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.1.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.1.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.1.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.1.output.dense.weight', 'task_models_list.1.auto.encoder.layer.1.output.dense.bias', 'task_models_list.1.auto.encoder.layer.1.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.1.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.2.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.2.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.2.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.2.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.2.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.2.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.2.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.2.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.2.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.2.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.2.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.2.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.2.output.dense.weight', 'task_models_list.1.auto.encoder.layer.2.output.dense.bias', 'task_models_list.1.auto.encoder.layer.2.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.2.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.3.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.3.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.3.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.3.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.3.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.3.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.3.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.3.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.3.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.3.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.3.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.3.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.3.output.dense.weight', 'task_models_list.1.auto.encoder.layer.3.output.dense.bias', 'task_models_list.1.auto.encoder.layer.3.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.3.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.4.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.4.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.4.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.4.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.4.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.4.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.4.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.4.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.4.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.4.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.4.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.4.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.4.output.dense.weight', 'task_models_list.1.auto.encoder.layer.4.output.dense.bias', 'task_models_list.1.auto.encoder.layer.4.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.4.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.5.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.5.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.5.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.5.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.5.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.5.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.5.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.5.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.5.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.5.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.5.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.5.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.5.output.dense.weight', 'task_models_list.1.auto.encoder.layer.5.output.dense.bias', 'task_models_list.1.auto.encoder.layer.5.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.5.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.6.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.6.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.6.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.6.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.6.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.6.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.6.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.6.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.6.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.6.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.6.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.6.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.6.output.dense.weight', 'task_models_list.1.auto.encoder.layer.6.output.dense.bias', 'task_models_list.1.auto.encoder.layer.6.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.6.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.7.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.7.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.7.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.7.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.7.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.7.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.7.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.7.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.7.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.7.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.7.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.7.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.7.output.dense.weight', 'task_models_list.1.auto.encoder.layer.7.output.dense.bias', 'task_models_list.1.auto.encoder.layer.7.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.7.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.8.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.8.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.8.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.8.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.8.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.8.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.8.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.8.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.8.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.8.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.8.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.8.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.8.output.dense.weight', 'task_models_list.1.auto.encoder.layer.8.output.dense.bias', 'task_models_list.1.auto.encoder.layer.8.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.8.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.9.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.9.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.9.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.9.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.9.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.9.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.9.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.9.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.9.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.9.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.9.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.9.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.9.output.dense.weight', 'task_models_list.1.auto.encoder.layer.9.output.dense.bias', 'task_models_list.1.auto.encoder.layer.9.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.9.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.10.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.10.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.10.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.10.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.10.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.10.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.10.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.10.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.10.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.10.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.10.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.10.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.10.output.dense.weight', 'task_models_list.1.auto.encoder.layer.10.output.dense.bias', 'task_models_list.1.auto.encoder.layer.10.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.10.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.11.attention.self.query.weight', 'task_models_list.1.auto.encoder.layer.11.attention.self.query.bias', 'task_models_list.1.auto.encoder.layer.11.attention.self.key.weight', 'task_models_list.1.auto.encoder.layer.11.attention.self.key.bias', 'task_models_list.1.auto.encoder.layer.11.attention.self.value.weight', 'task_models_list.1.auto.encoder.layer.11.attention.self.value.bias', 'task_models_list.1.auto.encoder.layer.11.attention.output.dense.weight', 'task_models_list.1.auto.encoder.layer.11.attention.output.dense.bias', 'task_models_list.1.auto.encoder.layer.11.attention.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.11.attention.output.LayerNorm.bias', 'task_models_list.1.auto.encoder.layer.11.intermediate.dense.weight', 'task_models_list.1.auto.encoder.layer.11.intermediate.dense.bias', 'task_models_list.1.auto.encoder.layer.11.output.dense.weight', 'task_models_list.1.auto.encoder.layer.11.output.dense.bias', 'task_models_list.1.auto.encoder.layer.11.output.LayerNorm.weight', 'task_models_list.1.auto.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['roberta.embeddings.position_ids', 'roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'classifier.weight', 'classifier.bias'].\n",
      "100%|██████████| 8/8 [00:11<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.3269, 'train_samples_per_second': 0.706, 'train_steps_per_second': 0.706, 'train_loss': 5.922636985778809, 'epoch': 1.0}\n",
      "[*] Evaluating task 0 => encoding_abs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Evaluating task 1 => encoding_rel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Calling save_model with task_idx 0\n",
      "[*] Initializing adapter with 2 classifiers\n",
      "[*] Predicting task 0 => encoding_abs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting task 1 => encoding_rel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing adapter with 2 classifiers\n",
      "['2[_]PP', '3[_]NP', '1[_]S', '1[_]S', '2[_]NP', '1[_]S', '1[_]S', '2[_]VP', '3[_]ADVP', '5[_]QP', '4[_]NP', '1[_]S', '1[_]S']\n",
      "['1[_]S', '2[_]PP', '3[_]NP', '3[_]NP', '3[_]NP', '1[_]S', '1[_]S', '1[_]S', '1[_]S', '2[_]NP', '2[_]NP', '2[_]NP', '2[_]NP', '1[_]S', '1[_]S', '1[_]S', '2[_]PP', '4[_]QP', '4[_]QP', '3[_]NP', '1[_]S', '2[_]VP', '2[_]VP', '5[_]QP', '4[_]NP', '3[_]NP', '4[_]PP', '6[_]NP', '5[_]NP', '5[_]NP', '6[_]NP', '2[_]VP', '2[_]VP', '4[_]S[+]VP', '4[_]S[+]VP', '4[_]S[+]VP[_]PRT', '6[_]NP', '7[_]ADJP[_]ADJP', '7[_]ADJP[_]ADJP', '7[_]ADJP[_]ADJP', '7[_]ADJP[_]ADJP', '7[_]ADJP[_]ADJP', '7[_]ADJP', '7[_]ADJP', '8[_]ADJP', '8[_]ADJP', '8[_]ADJP', '7[_]ADJP', '6[_]NP', '5[_]NP', '6[_]PP', '7[_]NP', '4[_]S[+]VP', '5[_]PP', '6[_]NP', '6[_]NP', '6[_]NP', '6[_]NP', '6[_]NP', '6[_]NP', '1[_]S', '1[_]S']\n",
      "['2[_]NP', '1[_]S', '1[_]S', '1[_]S', '1[_]S', '2[_]VP', '2[_]VP[_]ADVP', '2[_]VP[_]ADVP', '1[_]S[_]VP', '1[_]S[_]VP', '1[_]S']\n",
      "['2[_]S[_]NP[+]NP', '3[_]VP', '4[_]VP', '5[_]NP', '4[_]VP', '4[_]VP', '4[_]VP', '6[_]S[+]VP', '7[_]VP', '7[_]VP', '7[_]VP[_]NP', '7[_]VP[_]NP', '10[_]VP', '10[_]VP', '9[_]S[+]VP[_]VP', '9[_]S[+]VP', '10[_]VP', '11[_]VP', '12[_]PP', '13[_]NP', '13[_]NP', '13[_]NP', '13[_]NP', '15[_]S[+]VP', '1[_]S[_]VP', '1[_]S', '1[_]S', '2[_]NP', '2[_]NP', '1[_]S', '2[_]VP', '3[_]NP', '1[_]S', '1[_]S', '1[_]S']\n",
      "['1[_]S', '1[_]S[_]NP', '2[_]VP', '3[_]NP', '3[_]NP', '3[_]NP', '4[_]SBAR', '7[_]NP', '7[_]NP', '7[_]NP', '6[_]NP', '6[_]NP', '5[_]S[_]NP', '6[_]VP', '7[_]NP[_]NP', '7[_]NP', '9[_]NP', '8[_]NP', '9[_]PP', '1[_]S[_]NP', '1[_]S', '1[_]S']\n",
      "[*] Initializing adapter with 2 classifiers\n",
      "['3*[_]NP', '-1*[_]NP', '1*[_]PP', '2*[_]NP', '0*[_]NP', '0*[_]NP', '-1*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '-2*[_]NP', '-2*[_]NP', '-2*[_]NP', '1*[_]PP', '-2*[_]S[_]NP', '1*[_]VP[_]ADVP', '1*[_]VP[_]ADVP', '1*[_]VP[_]ADVP', '0*[_]VP', '0*[_]VP[_]NP', '-1*[_]S[_]ADVP', '-1*[_]S[_]ADVP', '0*[_]S']\n",
      "['1*[_]SINV', '3*[_]NP', '-1*[_]S', '1*[_]VP', '-2*[_]S[_]ADJP', '0*[_]S', '0*[_]S', '1*[_]S[_]NP', '1*[_]VP', '2*[_]NP', '0*[_]NP', '0*[_]NP', '-1*[_]NP', '0*[_]NP', '1*[_]SBAR', '2*[_]S[+]VP', '1*[_]VP', '1*[_]VP', '1*[_]PP', '2*[_]NP', '2*[_]NP', '2*[_]NP', '2*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '-1*[_]NP', '0*[_]NP', '-11*[_]SINV', '-11*[_]SINV', '0*[_]SINV', '0*[_]SINV[_]VP', '1*[_]NP', '1*[_]NP', '-1*[_]SINV', '-1*[_]SINV', '1*[_]PP', '1*[_]NP', '-2*[_]SINV', '-2*[_]SINV', '0*[_]SINV', '0*[_]SINV']\n",
      "['2*[_]PP', '1*[_]NP', '-2*[_]S', '0*[_]S', '0*[_]S[_]NP', '1*[_]VP', '1*[_]VP', '0*[_]VP[_]PRT', '2*[_]S[+]VP', '1*[_]VP', '1*[_]VP', '1*[_]VP', '1*[_]VP', '1*[_]PP', '2*[_]NP', '-1*[_]NP', '-1*[_]NP', '-1*[_]NP', '-1*[_]NP', '0*[_]NP', '1*[_]SBAR[_]WHNP', '3*[_]VP', '0*[_]VP[_]PRT', '0*[_]VP[_]NP', '1*[_]PP', '2*[_]S[+]VP', '2*[_]S[+]VP', '0*[_]S[+]VP[_]NP', '0*[_]S[+]VP[_]NP', '0*[_]S[+]VP[_]NP', '1*[_]SBAR[_]WHADVP', '2*[_]NP', '2*[_]NP', '2*[_]NP', '-1*[_]S', '-1*[_]S', '-6*[_]S[+]VP[_]VP', '0*[_]S[+]VP', '0*[_]S[+]VP', '1*[_]VP[_]ADVP', '1*[_]VP[_]ADVP', '1*[_]VP[_]ADVP', '0*[_]VP', '0*[_]VP[_]PRT', '-12*[_]S[_]NP', '0*[_]S']\n",
      "['2*[_]NP', '0*[_]NP', '0*[_]NP', '-1*[_]S', '-1*[_]S', '-1*[_]S', '1*[_]VP', '1*[_]ADVP', '1*[_]NP', '-2*[_]VP', '1*[_]PP', '1*[_]NP', '-2*[_]VP', '-2*[_]VP', '-2*[_]VP', '-2*[_]VP', '1*[_]PP', '1*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '-3*[_]S', '-3*[_]S', '0*[_]S']\n",
      "['2*[_]NP', '-1*[_]S', '1*[_]VP', '4*[_]NP', '-1*[_]NP', '1*[_]PP', '2*[_]NP', '1*[_]QP', '0*[_]QP', '-2*[_]NP', '1*[_]VP', '1*[_]VP', '1*[_]PP', '1*[_]NP', '-6*[_]SBAR[+]S', '-6*[_]SBAR[+]S', '1*[_]VP', '1*[_]VP', '1*[_]VP', '2*[_]S[+]VP', '1*[_]VP', '1*[_]VP', '2*[_]NP', '0*[_]NP', '-1*[_]NP', '-1*[_]NP', '0*[_]NP', '2*[_]NP', '-1*[_]NP', '-1*[_]NP', '-1*[_]NP', '1*[_]VP', '1*[_]VP', '1*[_]PP', '1*[_]NP', '1*[_]ADJP', '-1*[_]NP', '-1*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '0*[_]NP', '-14*[_]S', '-14*[_]S', '0*[_]S']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Sequence\n",
    "from datasets import ClassLabel\n",
    "from hfmtl.tasks.sequence_classification import SequenceClassification\n",
    "from hfmtl.tasks.token_classification import TokenClassification\n",
    "from hfmtl.utils import *\n",
    "from hfmtl.models import *\n",
    "\n",
    "from codelin.models.const_tree import C_Tree\n",
    "from codelin.encs.constituent import *\n",
    "from codelin.utils.constants import *\n",
    "\n",
    "import easydict\n",
    "from frozendict import frozendict\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ptb_path = \"~/Treebanks/const/PENN_TREEBANK/\"\n",
    "ptb_path = os.path.expanduser(ptb_path)\n",
    "\n",
    "\n",
    "def generate_dataset_from_codelin(train_dset, dev_dset, test_dset):\n",
    "    # Get all possible labels and cast to ClassLabel\n",
    "    # Get all possible labels and cast to ClassLabel\n",
    "    label_set = set()\n",
    "    for dset in [train_dset, dev_dset, test_dset]:\n",
    "        for labels in dset[\"target\"]:\n",
    "            label_set.update(labels)\n",
    "    label_names = sorted(list(label_set))\n",
    "\n",
    "    train_dset = datasets.Dataset.from_dict(train_dset)\n",
    "    train_dset = train_dset.cast_column(\"target\", Sequence(ClassLabel(num_classes=len(label_names), names=label_names)))\n",
    "\n",
    "    dev_dset = datasets.Dataset.from_dict(dev_dset)\n",
    "    dev_dset = dev_dset.cast_column(\"target\", Sequence(ClassLabel(num_classes=len(label_names), names=label_names)))\n",
    "\n",
    "    test_dset = datasets.Dataset.from_dict(test_dset)\n",
    "    test_dset = test_dset.cast_column(\"target\", Sequence(ClassLabel(num_classes=len(label_names), names=label_names)))\n",
    "    \n",
    "    # Convert to Hugging Face DatasetDict format\n",
    "    dataset = datasets.DatasetDict({\n",
    "            \"train\": train_dset,\n",
    "            \"validation\": dev_dset,\n",
    "            \"test\": test_dset\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def encode_dset(encoder, dset):\n",
    "    encoded_trees = {\"tokens\":[], \"target\":[]}\n",
    "    for line in dset:\n",
    "        tree = C_Tree.from_string(line)\n",
    "        lin_tree = encoder.encode(tree)\n",
    "        encoded_trees[\"tokens\"].append([w for w in lin_tree.words])\n",
    "        encoded_trees[\"target\"].append([str(l) for l in lin_tree.labels])\n",
    "    \n",
    "    return encoded_trees\n",
    "\n",
    "with open(os.path.join(ptb_path,\"test.trees\")) as f:\n",
    "    ptb_test = [l.rstrip() for l in f.read().splitlines()]\n",
    "with open(os.path.join(ptb_path,\"dev.trees\")) as f:\n",
    "    ptb_dev = [l.rstrip() for l in f.read().splitlines()]\n",
    "with open(os.path.join(ptb_path,\"train.trees\")) as f:\n",
    "    ptb_train = [l.rstrip() for l in f.read().splitlines()]\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"max_seq_length\": 128,\n",
    "    \n",
    "    \"batch_size\": 32,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "\n",
    "    \"num_train_epochs\": 1,\n",
    "\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.999,\n",
    "    \"max_seq_length\": 64,\n",
    "    \"do_eval\": True,\n",
    "    \"do_train\": True,\n",
    "    \"do_predict\": True,\n",
    "    \n",
    "    \n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"logging_steps\": 100,\n",
    "    \n",
    "    \n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"batch_truncation\": True,\n",
    "    \"add_cls\": True,\n",
    "    \"add_clf\": True,\n",
    "    \"drop_probability\": 0.1,\n",
    "    \"include_inputs_for_metrics\": True,\n",
    "    \"model_name\": \"roberta-base\"\n",
    "    })\n",
    "\n",
    "\n",
    "encoder = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "dataset_abs = generate_dataset_from_codelin(encode_dset(encoder, ptb_train[:100]), encode_dset(encoder, ptb_dev[:100]), encode_dset(encoder, ptb_test[:100]))\n",
    "\n",
    "encoder = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "dataset_rel = generate_dataset_from_codelin(encode_dset(encoder, ptb_train[:100]), encode_dset(encoder, ptb_dev[:100]), encode_dset(encoder, ptb_test[:100]))\n",
    "\n",
    "tasks = [TokenClassification(\n",
    "            dataset = dataset_abs,\n",
    "            name = \"encoding_abs\",\n",
    "            tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=args.max_seq_length, truncation=True)\n",
    "        ),\n",
    "        TokenClassification(\n",
    "            dataset = dataset_rel,\n",
    "            name = \"encoding_rel\",\n",
    "            tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=args.max_seq_length, truncation=True)\n",
    "        )]\n",
    "        \n",
    "model   = Model(tasks, args) # list of models; by default, shared encoder, task-specific CLS token task-specific head \n",
    "trainer = Trainer(model, tasks, args) # tasks are uniformly sampled by default\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model(\"./saved\")\n",
    "test_prediction = trainer.predict(None)\n",
    "\n",
    "for i, t in enumerate(model.task_names):\n",
    "    pred_i = test_prediction[t]\n",
    "    model.factorize(task_index = i)\n",
    "    id2label = model.task_models_list[i].config.id2label\n",
    "    for p in (pred_i[:5]):\n",
    "        labels = []\n",
    "        for l in p:\n",
    "            if l!=-100:\n",
    "                labels.append(id2label[l])\n",
    "        print(labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
