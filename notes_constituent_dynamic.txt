Better, Faster, Stronger Sequence Tagging Constituent Parsers

Factors that prevent sequence tagging constituent parser from obtaining better results:

    -> High error rates when long constituents need to be closed
        f-scores indicate that high values on relative tagging (specially negative values) are bad predicted
    -> Label sparsity
        Ptb gives a total of 1500 labels, where 60% are ocurring less than 5 times, being difficult to predict
    -> Error propagation arising from greedy inference

Preliminaries

    A encoded label is

            (n, c, u)

    where

            n = commons number ancestors (encoded as relative or absolute)
            c = last common ancestor
            u = pos unary chain

Solution: Combine relative tagging with absolute tagging, allowing to train a model that switchs between two encodings

    Relative encoding is effective to predict the beginning and end of short constituents (nt <= 2, being nt the variation respect nt-1)
    Relative encoding is not effective when large constituent must be closed

    Absolute encoding is useful because a pair of words that share the same m top levels will be equally encoded
    Absolute encoding is sparse when predicting deep levels

    Therefore

    Dynamic encoding:

        Use relative by default

        Uses absolute iff

            absolute encoding: wt share at most the three top levels with wt+1

                    abs_enc(w,w+1)=(n,c,u) and n <=3

            and relative encoding: wt is at least two levels deeper in the tree than wt+1

                    rel_enc(w,w+1)=(n,c,u) and n <=-2

Note: Absolute encoding is also called top-down absolute scale encoding

Why LSTMs?
    -> Sequence tagging is a structured prediction task and lstms are state of the art performance for such tasks
Why?
    -> Because in lstms the prediction for the ith element is conditioned on the output of previous steps

