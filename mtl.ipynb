{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Sequence\n",
    "from datasets import ClassLabel\n",
    "\n",
    "from PYEVALB.scorer import Scorer\n",
    "from PYEVALB.summary import summary\n",
    "\n",
    "from codelin.models.const_tree import C_Tree\n",
    "from codelin.models.const_label import C_Label\n",
    "from codelin.models.linearized_tree import LinearizedTree\n",
    "from codelin.encs.constituent import *\n",
    "from codelin.utils.constants import *\n",
    "\n",
    "from frozendict import frozendict\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set logging level\n",
    "'''\n",
    "Train the models in multi-task learning fashion. To do this\n",
    "we will split the fields of the label and train different\n",
    "tasks according to this. After training, we will evaluate\n",
    "the decoded trees by re-joining the labels.\n",
    "'''\n",
    "\n",
    "ptb_path = \"~/Treebanks/const/PENN_TREEBANK/\"\n",
    "ptb_path = os.path.expanduser(ptb_path)\n",
    "\n",
    "with open(os.path.join(ptb_path,\"test.trees\")) as f:\n",
    "    ptb_test = [l.rstrip() for l in f.read().splitlines()]\n",
    "with open(os.path.join(ptb_path,\"dev.trees\")) as f:\n",
    "    ptb_dev = [l.rstrip() for l in f.read().splitlines()]\n",
    "with open(os.path.join(ptb_path,\"train.trees\")) as f:\n",
    "    ptb_train = [l.rstrip() for l in f.read().splitlines()]\n",
    "\n",
    "def get_n_labels(dsets, tar_field):\n",
    "    label_set = set()\n",
    "    for dset in dsets:\n",
    "        for labels in dset[tar_field]:\n",
    "            label_set.update(labels)\n",
    "    label_names = sorted(list(label_set))\n",
    "    return label_names, len(label_names)\n",
    "\n",
    "def generate_dataset_from_codelin(train_dset, dev_dset, test_dset=None):\n",
    "    dsets = [train_dset, dev_dset, test_dset] if test_dset else [train_dset, dev_dset]\n",
    "    \n",
    "    l1, nl1 = get_n_labels(dsets, \"target_1\")\n",
    "    l2, nl2 = get_n_labels(dsets, \"target_2\")\n",
    "    l3, nl3 = get_n_labels(dsets, \"target_3\")\n",
    "\n",
    "    train_dset = datasets.Dataset.from_dict(train_dset)\n",
    "    train_dset = train_dset.cast_column(\"target_1\", Sequence(ClassLabel(num_classes=nl1, names=l1)))\n",
    "    train_dset = train_dset.cast_column(\"target_2\", Sequence(ClassLabel(num_classes=nl2, names=l2)))\n",
    "    train_dset = train_dset.cast_column(\"target_3\", Sequence(ClassLabel(num_classes=nl3, names=l3)))\n",
    "\n",
    "    dev_dset = datasets.Dataset.from_dict(dev_dset)\n",
    "    dev_dset = dev_dset.cast_column(\"target_1\", Sequence(ClassLabel(num_classes=nl1, names=l1)))\n",
    "    dev_dset = dev_dset.cast_column(\"target_2\", Sequence(ClassLabel(num_classes=nl2, names=l2)))\n",
    "    dev_dset = dev_dset.cast_column(\"target_3\", Sequence(ClassLabel(num_classes=nl3, names=l3)))\n",
    "\n",
    "    if test_dset:\n",
    "        test_dset = datasets.Dataset.from_dict(test_dset)\n",
    "        test_dset = test_dset.cast_column(\"target_1\", Sequence(ClassLabel(num_classes=nl1, names=l1)))\n",
    "        test_dset = test_dset.cast_column(\"target_2\", Sequence(ClassLabel(num_classes=nl2, names=l2)))\n",
    "        test_dset = test_dset.cast_column(\"target_3\", Sequence(ClassLabel(num_classes=nl3, names=l3)))\n",
    "    \n",
    "        # Convert to Hugging Face DatasetDict format\n",
    "        dataset = datasets.DatasetDict({\n",
    "                \"train\": train_dset,\n",
    "                \"validation\": dev_dset,\n",
    "                \"test\": test_dset\n",
    "            })\n",
    "    else:\n",
    "        # Convert to Hugging Face DatasetDict format\n",
    "        dataset = datasets.DatasetDict({\n",
    "                \"train\": train_dset,\n",
    "                \"validation\": dev_dset\n",
    "            })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def encode_dset(encoder, dset):\n",
    "    encoded_trees = {\"tokens\":[], \"target_1\":[], \"target_2\":[], \"target_3\":[]}\n",
    "    max_len_tree = 0\n",
    "    for line in dset:\n",
    "        tree = C_Tree.from_string(line)\n",
    "        lin_tree = encoder.encode(tree)\n",
    "        encoded_trees[\"tokens\"].append([w for w in lin_tree.words])\n",
    "        \n",
    "        t1,t2,t3 = [],[],[]\n",
    "        for s1,s2,s3 in lin_tree.get_labels_splitted():\n",
    "            t1.append(s1)    \n",
    "            t2.append(s2)\n",
    "            t3.append(s3)\n",
    "            \n",
    "        encoded_trees[\"target_1\"].append(t1)\n",
    "        encoded_trees[\"target_2\"].append(t2)\n",
    "        encoded_trees[\"target_3\"].append(t3)\n",
    "        \n",
    "        max_len_tree = max(max_len_tree, len(lin_tree.words))\n",
    "    \n",
    "    \n",
    "    return encoded_trees, max_len_tree\n",
    "\n",
    "def gen_dsets():\n",
    "    encodings = []\n",
    "\n",
    "    # naive absolute encodings\n",
    "    a_enc     = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute\", \"encoder\":a_enc})\n",
    "    a_br_enc  = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_br\", \"encoder\":a_br_enc})\n",
    "    a_bl_enc  = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_bl\", \"encoder\":a_bl_enc})\n",
    "    ar_enc    = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_r\", \"encoder\":ar_enc})\n",
    "    ar_br_enc = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_r_br\", \"encoder\":ar_br_enc})\n",
    "    ar_bl_enc = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_r_bl\", \"encoder\":ar_bl_enc})\n",
    "\n",
    "    # naive relative encodings\n",
    "    r_enc     = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative\", \"encoder\":r_enc})\n",
    "    r_br_enc  = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_br\", \"encoder\":r_br_enc})\n",
    "    r_bl_enc  = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_bl\", \"encoder\":r_bl_enc})\n",
    "    rr_enc    = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_r\", \"encoder\":rr_enc})\n",
    "    rr_br_enc = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_r_br\", \"encoder\":rr_br_enc})\n",
    "    rr_bl_enc = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_r_bl\", \"encoder\":rr_bl_enc})\n",
    "\n",
    "    # naive dynamic encodings\n",
    "    d_enc     = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic\", \"encoder\":d_enc})\n",
    "    d_br_enc  = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_br\", \"encoder\":d_br_enc})\n",
    "    d_bl_enc  = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_bl\", \"encoder\":d_bl_enc})\n",
    "    dr_enc    = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_r\", \"encoder\":dr_enc})\n",
    "    dr_br_enc = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_r_br\", \"encoder\":dr_br_enc})\n",
    "    dr_bl_enc = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_r_bl\", \"encoder\":dr_bl_enc})\n",
    "\n",
    "    # gaps encodings\n",
    "    g_r_enc   = C_GapsEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary_direction=\"R\", binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"gaps_r\", \"encoder\":g_r_enc})\n",
    "    g_l_enc   = C_GapsEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary_direction=\"L\", binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"gaps_l\", \"encoder\":g_l_enc})\n",
    "\n",
    "    # tetra encodings\n",
    "    t_pr_enc  = C_Tetratag(separator=\"[_]\", unary_joiner=\"[+]\", mode='preorder',  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"tetratag_preorder\", \"encoder\":t_pr_enc})\n",
    "    t_in_enc  = C_Tetratag(separator=\"[_]\", unary_joiner=\"[+]\", mode='inorder',   binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"tetratag_inorder\", \"encoder\":t_in_enc})\n",
    "    t_po_enc  = C_Tetratag(separator=\"[_]\", unary_joiner=\"[+]\", mode='postorder', binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"tetratag_postorder\", \"encoder\":t_po_enc})\n",
    "\n",
    "    # yuxtaposed encodings\n",
    "    j_enc   = C_JuxtaposedEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"juxtaposed\", \"encoder\":j_enc})\n",
    "    j_r_enc = C_JuxtaposedEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary=True, binary_direction='R',   binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"juxtaposed_r\", \"encoder\":j_r_enc})\n",
    "    j_l_enc = C_JuxtaposedEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary=True, binary_direction='L',   binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"juxtaposed_l\", \"encoder\":j_l_enc})\n",
    "\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TokenClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels = None, attention_mask = None, **kwargs):\n",
    "        sequence_output_dropout = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output_dropout)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            labels = labels.long()\n",
    "\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss,\n",
    "                    labels.view(-1),\n",
    "                    torch.tensor(loss_fct.ignore_index).type_as(labels),\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are processing is composed of \n",
    "- Task: Each of the datasets that we are employing \n",
    "- Subtasks: Each of the labels we are predicting for each word in the task dataset\n",
    "\n",
    "For example:\n",
    "\n",
    "- Task-1: The, dog, is, brown\n",
    "    - Subtask-1: DT, NOUN, VERB, ADJ\n",
    "    - Subtask-2: 1_NP, 2_VP, 1_ADJ, 1_NP\n",
    "    - Subtask-3: <_det, <//_root, >_whatever, //_mod\n",
    "<br><br>\n",
    "\n",
    "- Task-2: El, perro, es, marron\n",
    "    - Subtask-1: DT, NOUN, VERB, ADJ\n",
    "    - Subtask-2: 1_NP, 2_VP, 1_ADJ, 1_NP\n",
    "\n",
    "An example of this would be:\n",
    "\n",
    "```\n",
    "{\n",
    "    'naive_absolute_ptb': \n",
    "    {\n",
    "            'train': \n",
    "                    Dataset({\n",
    "                        features: ['tokens', 'target_1', 'target_2', 'input_ids', 'token_type_ids', 'attention_mask', 'task_ids'],\n",
    "                        num_rows: 256\n",
    "                    }), \n",
    "            'validation': \n",
    "                    Dataset({\n",
    "                        features: ['tokens', 'target_1', 'target_2', 'target_3', 'input_ids', 'token_type_ids', 'attention_mask', 'task_ids'],\n",
    "                        num_rows: 256\n",
    "                    })\n",
    "    },\n",
    "    'naive_absolute_en_ewt': \n",
    "    {\n",
    "            'train': \n",
    "                    Dataset({\n",
    "                        features: ['tokens', 'target_1', 'target_2', 'input_ids', 'token_type_ids', 'attention_mask', 'task_ids'],\n",
    "                        num_rows: 256\n",
    "                    }), \n",
    "            'validation': \n",
    "                    Dataset({\n",
    "                        features: ['tokens', 'target_1', 'target_2', 'input_ids', 'token_type_ids', 'attention_mask', 'task_ids'],\n",
    "                        num_rows: 256\n",
    "                    })\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Where we are training the fields of ```head_pos``` (taget_1) and ```dep_rel``` (target_2) for dependency parsing casted as sequence labeling for two different datasets (penn treebank and english web treebank). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import EvalPrediction\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import RandomSampler, WeightedRandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers.data.data_collator import InputDataClass\n",
    "from types import MappingProxyType\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import transformers\n",
    "from transformers.trainer_utils import EvalLoopOutput\n",
    "\n",
    "class DataLoaderWithTaskname:\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task = task_name\n",
    "        self.data_loader = data_loader\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            yield batch\n",
    "\n",
    "class NLPDataCollator:\n",
    "    def __init__(self, tasks):\n",
    "        self.tasks = tasks\n",
    "\n",
    "    def __call__(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
    "        features = [{k:v for k,v in x.items() if k!='task_ids'} for x in features]\n",
    "        return features\n",
    "    \n",
    "class MultitaskDataloader:\n",
    "    \"\"\"\n",
    "    Data loader that combines and samples from multiple single-task\n",
    "    data loaders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataloader_dict, p=1):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        N = max([len(x)**(1-p) for x in dataloader_dict.values()])\n",
    "        \n",
    "        f_p = lambda x: int(N*x**p)\n",
    "\n",
    "        self.num_batches_dict = {\n",
    "            task_name: f_p(len(dataloader))\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            f_p(len(dataloader.dataset)) for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and yield a batch from the respective\n",
    "        task Dataloader.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        \n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader)\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, encoder_name_or_path, tasks):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name_or_path)\n",
    "        tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name_or_path, **tokenizer_kwargs)\n",
    "        self.output_heads = nn.ModuleDict()\n",
    "        \n",
    "        for task in tasks:\n",
    "            task.set_tokenizer(self.tokenizer)\n",
    "            for subtask in task.y:\n",
    "                decoder = self._create_output_head(\n",
    "                    self.encoder.config.hidden_size, \n",
    "                    task.task_type, \n",
    "                    task.num_labels[subtask]\n",
    "                )\n",
    "                \n",
    "                self.output_heads[subtask] = decoder\n",
    "\n",
    "        self.processed_tasks = self.preprocess_tasks(tasks, self.tokenizer)\n",
    "        self.label_names = {task.name: task.label_names for task in tasks}\n",
    "        self.train_dataset = {self.processed_tasks[task.name]['train'] for task in tasks}\n",
    "        self.eval_dataset = {self.processed_tasks[task.name]['validation'] for task in tasks}\n",
    "    \n",
    "    def preprocess_tasks(self, tasks, tokenizer):      \n",
    "        features_dict = {}\n",
    "        for i, task in enumerate(tasks):\n",
    "            print(\"[*] Model preprocessing task\", task.name)\n",
    "            \n",
    "            if hasattr(task, 'processed_features') and tokenizer == task.tokenizer:\n",
    "                features_dict[task.name] = task.processed_features\n",
    "                continue\n",
    "            \n",
    "            for split in task.dataset:\n",
    "                task.index = task.dataset[split].index = i\n",
    "            \n",
    "            features_dict[task.name] = {}\n",
    "            for phase, phase_dataset in task.dataset.items():\n",
    "                phase_dataset.index = i\n",
    "\n",
    "                features_dict[task.name][phase] = phase_dataset.map(\n",
    "                    task.preprocess_function, \n",
    "                    batched = True,\n",
    "                    batch_size = 8,\n",
    "                    load_from_cache_file = True\n",
    "                )\n",
    "        return features_dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_output_head(encoder_hidden_size: int, task_type, n_labels):\n",
    "        if task_type == \"TokenClassification\":\n",
    "            print(\"[*] Creating TokenClassification head with\", n_labels, \"labels\")\n",
    "            return TokenClassificationHead(encoder_hidden_size, n_labels)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_ids = None, attention_mask = None, token_type_ids = None, position_ids = None,\n",
    "            head_mask = None, inputs_embeds = None, labels = None, task_ids = None, **kwargs):\n",
    "            print(\"FORWARD MODEL FORWARD MODEL FORWARD MODEL\")\n",
    "            # compute the transformer output\n",
    "            # this is never called?\n",
    "            outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "            sequence_output, pooled_output = outputs[:2]\n",
    "\n",
    "            print(\"3) Transformer has been forwarded\")\n",
    "            unique_task_ids_list = torch.unique(task_ids).tolist()\n",
    "\n",
    "            loss_list = []\n",
    "            logits = None\n",
    "            # print(\"Computing loss...\")\n",
    "            # print(\"task_ids\", task_ids)\n",
    "            print(\"==> I have to compute loss for the following tasks:\")\n",
    "            print(\"==>\", unique_task_ids_list)\n",
    "            for unique_task_id in unique_task_ids_list:\n",
    "                print(\"Task_id =\",unique_task_id)\n",
    "                ptc_train = self.processed_tasks['train']\n",
    "                target_cols = [col for col in ptc_train.features if col.startswith(\"target_\")]\n",
    "                print(\"target_cols =\", target_cols)\n",
    "\n",
    "                # for tc in target_cols:\n",
    "                #     print(\"Target Column =\",tc)\n",
    "                #     print(\"Labels =\",labels)\n",
    "                #     logits, task_loss = self.output_heads[str(unique_task_id)].forward(\n",
    "                #         sequence_output[task_id_filter],\n",
    "                #         pooled_output[task_id_filter],\n",
    "                #         labels = None if labels is None else labels[task_id_filter],\n",
    "                #         attention_mask=attention_mask[task_id_filter],\n",
    "                #     )\n",
    "\n",
    "                #     if labels is not None:\n",
    "                #         loss_list.append(task_loss)\n",
    "\n",
    "            # Loss averaged over all tasks\n",
    "            outputs = (logits, outputs[2:])\n",
    "            if loss_list:\n",
    "                loss = torch.stack(loss_list)\n",
    "                outputs = (loss.mean(),) + outputs\n",
    "\n",
    "            return outputs\n",
    "\n",
    "\n",
    "class MultiTaskTrainer(transformers.Trainer):\n",
    "    def __init__(self, tasks, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.p = 1\n",
    "        self.processed_tasks = self.model.processed_tasks\n",
    "        self.label_names = self.model.label_names\n",
    "        self.train_dataset = {\n",
    "            task: dataset[\"train\"]\n",
    "            for task, dataset in self.processed_tasks.items()\n",
    "        }\n",
    "        self.eval_dataset = {\n",
    "            task: dataset[\"validation\"]\n",
    "            for task, dataset in self.processed_tasks.items()\n",
    "        }\n",
    "        self.eval_dataset = MappingProxyType(self.eval_dataset)\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.pretrained_transformer = self.model.encoder\n",
    "        self.device = self.pretrained_transformer.device\n",
    "        self.data_collator = NLPDataCollator(tasks)\n",
    "        \n",
    "        print(\"[*] Init multitask trainer with tasks:\", self.processed_tasks)\n",
    "        print(\"[*] Label names are:\", self.label_names)\n",
    "           \n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        \n",
    "        train_sampler = (SequentialSampler(train_dataset) if self.args.local_rank == -1 else DistributedSampler(train_dataset))\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name = task_name,\n",
    "            data_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size = self.args.train_batch_size,\n",
    "                shuffle = False,\n",
    "                sampler = train_sampler,\n",
    "                collate_fn = self.data_collator.__call__,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        return MultitaskDataloader(\n",
    "            {\n",
    "                task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "                for task_name, task_dataset in self.train_dataset.items()\n",
    "            }, p = self.p,\n",
    "        )\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        return MultitaskDataloader(\n",
    "            {\n",
    "                task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "                for task_name, task_dataset in (\n",
    "                    eval_dataset if eval_dataset else self.eval_dataset\n",
    "                ).items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def evaluation_loop(self, dataloader: DataLoader, description: str, \n",
    "                        prediction_loss_only: bool | None = None, ignore_keys: List[str] | None = None, \n",
    "                        metric_key_prefix: str = \"eval_\") -> EvalLoopOutput:\n",
    "          \n",
    "        # this is useless?\n",
    "        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "\n",
    "        for step, inputs in enumerate(dataloader):            \n",
    "            loss, preds, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "            \n",
    "            for task, label_names in self.label_names.items():\n",
    "                preds_task = preds[task]\n",
    "                labels_task = labels[task]\n",
    "                \n",
    "                for label_name, labels_values in label_names.items():\n",
    "                    preds_tl  = preds_task[label_name]\n",
    "                    labels_tl = labels_task[label_name]\n",
    "                    \n",
    "                    eval_pred = EvalPrediction(\n",
    "                                predictions = preds_tl, \n",
    "                                label_ids   = labels_tl, \n",
    "                                inputs      = inputs)\n",
    "\n",
    "                    # compute metrics foreach head using the corresponding task eval_function\n",
    "                    # i copied the function from the task-specific class to this one\n",
    "                    metrics = self.compute_metrics_token_classification(eval_pred, label_name)\n",
    "                    metrics_eval = {}\n",
    "                    for metric in metrics.items():\n",
    "                        metrics_eval[metric_key_prefix + \"_\" + metric[0]] = metric[1]\n",
    "\n",
    "        return EvalLoopOutput(predictions=preds_tl, label_ids=labels_tl, metrics=metrics_eval, num_samples=len(self.eval_dataset))\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=[]):\n",
    "        if ignore_keys is None:\n",
    "            ignore_keys = []\n",
    "\n",
    "        loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "        loss = loss.mean().detach()\n",
    "        \n",
    "        logits_dict = {}\n",
    "        labels_dict = {}\n",
    "        for task_name, label_names in self.label_names.items():\n",
    "            logits_dict[task_name] = {}\n",
    "            labels_dict[task_name] = {}\n",
    "            for label_name in label_names:\n",
    "                logits_dict[task_name][label_name] = outputs[label_name]\n",
    "                logits_dict[task_name][label_name] = np.argmax(outputs[label_name].detach().cpu().numpy(), axis=2)\n",
    "                target_labels = []\n",
    "                for i in inputs:\n",
    "                    target_labels.append(i[label_name])\n",
    "                labels_dict[task_name][label_name] = torch.tensor(target_labels)\n",
    "        \n",
    "        return (loss, logits_dict, labels_dict)\n",
    "\n",
    "    def compute_metrics_token_classification(self, eval_pred, label_names):\n",
    "        predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "        \n",
    "        # for task in tasks\n",
    "        for task_name in self.label_names:\n",
    "            task_name = self.label_names['naive_absolute_n_commons']\n",
    "            true_labels = [\n",
    "                [task_name[label_names][int(l)] for l in label if l != -100] for label in labels\n",
    "            ]\n",
    "            \n",
    "            true_predictions = [\n",
    "                [task_name[label_names][p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            metric = evaluate.load(\"seqeval\")\n",
    "            all_metrics = metric.compute(\n",
    "                predictions = true_predictions, \n",
    "                references = true_labels\n",
    "            )\n",
    "            \n",
    "            meta = {\"name\": task_name, \"size\": len(predictions), \"index\": 0}\n",
    "            print(all_metrics)\n",
    "            metrics = {k.replace(\"overall_\",\"\"):v for k,v in all_metrics.items() if \"overall\" in k}\n",
    "        \n",
    "        print(metrics)\n",
    "        return {**metrics, **meta}      \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        keys = inputs[0].keys()\n",
    "\n",
    "        input_ids = torch.tensor([i['input_ids'] for i in inputs], device=self.args.device) if 'input_ids' in keys else None\n",
    "        attention_mask = torch.tensor([i['attention_mask'] for i in inputs], device=self.args.device) if 'attention_mask' in keys else None        \n",
    "        token_type_ids = torch.tensor([i['token_type_ids'] for i in inputs], device=self.args.device) if 'token_type_ids' in keys else None        \n",
    "        position_ids = torch.tensor([i['position_ids'] for i in inputs], device=self.args.device) if 'position_ids' in keys else None        \n",
    "        head_mask = torch.tensor([i['head_mask'] for i in inputs], device=self.args.device) if 'head_mask' in keys else None        \n",
    "        inputs_embeds = torch.tensor([i['inputs_embeds'] for i in inputs], device=self.args.device) if 'inputs_embeds' in keys else None\n",
    "        \n",
    "        outputs = self.pretrained_transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids,\n",
    "            head_mask = head_mask,\n",
    "            inputs_embeds = inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        loss_list = []\n",
    "        logits_list = {}\n",
    "        \n",
    "        for i, head in enumerate(self.model.output_heads.values()):\n",
    "            labels_name = f\"target_{i+1}\"\n",
    "            labels_i = torch.tensor([i[labels_name] for i in inputs], device=self.args.device)\n",
    "            logits, loss = head(sequence_output, pooled_output, labels=labels_i, attention_mask=attention_mask)\n",
    "            loss_list.append(loss)\n",
    "            logits_list[labels_name] = logits\n",
    "        \n",
    "        loss = torch.stack(loss_list)\n",
    "        return (loss, logits_list) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import funcy as fc\n",
    "import warnings\n",
    "from frozendict import frozendict as fdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TokenClassification:\n",
    "    task_type = \"TokenClassification\"\n",
    "    name: str = \"TokenClassificationTask\"\n",
    "    dataset: Dataset = None\n",
    "    metric:... = evaluate.load(\"seqeval\")\n",
    "    main_split: str = \"train\"\n",
    "    tokens: str = 'tokens'\n",
    "    y: str|list = 'target'\n",
    "    num_labels: int = None\n",
    "    label_names: dict = None\n",
    "    tokenizer_kwargs: fdict = fdict(padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _align_labels_with_tokens(labels, word_ids):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                new_labels.append(-100)\n",
    "\n",
    "            elif word_id != current_word:\n",
    "                current_word = word_id\n",
    "                label = -100 if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "            \n",
    "            else:\n",
    "                label = labels[word_id]\n",
    "                new_labels.append(label)\n",
    "        \n",
    "        return new_labels\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.label_names = {}\n",
    "        self.num_labels  = {}\n",
    "\n",
    "        for y in self.y:\n",
    "            target = self.dataset[self.main_split].features[y]\n",
    "            self.num_labels[y] = target.feature.num_classes\n",
    "            self.label_names[y] = target.feature.names if target.feature.names else [None]\n",
    "        \n",
    "        print(f\"[*] TokenClassificationTask loaded {self.task_type} task with {self.num_labels} labels\")\n",
    "        for k,v in self.label_names.items():\n",
    "            print(f\"      {k} labels: {v}\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return super().get_labels() or self.label_names\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.add_prefix_space = True\n",
    "        self.data_collator = DataCollatorForTokenClassification(\n",
    "            tokenizer = self.tokenizer\n",
    "        )\n",
    "\n",
    "    def preprocess_function(self, examples):\n",
    "        if examples[self.tokens] and type(examples[self.tokens][0]) == str:\n",
    "            unsqueeze, examples = True, {k:[v] for k,v in examples.items()}\n",
    "        \n",
    "        def get_len(outputs):\n",
    "            try:\n",
    "                return len(outputs[fc.first(outputs)])\n",
    "            except:\n",
    "                return 1\n",
    "        \n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            examples[self.tokens],\n",
    "            is_split_into_words=True,\n",
    "            **self.tokenizer_kwargs\n",
    "        )\n",
    "\n",
    "        for target_column in self.y:\n",
    "            all_labels = examples[target_column]\n",
    "            new_labels = []\n",
    "            \n",
    "            for i, labels in enumerate(all_labels):\n",
    "                word_ids = tokenized_inputs.word_ids(i)\n",
    "                new_labels.append(self._align_labels_with_tokens(labels, word_ids))\n",
    "            \n",
    "            tokenized_inputs[target_column] = new_labels        \n",
    "            tokenized_inputs['task_ids'] = [self.index]*get_len(tokenized_inputs)\n",
    "\n",
    "        return tokenized_inputs       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_dsets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# train and evaluate using Evalb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m encodings \u001b[39m=\u001b[39m gen_dsets()\n\u001b[1;32m      9\u001b[0m results \u001b[39m=\u001b[39m {}\n\u001b[1;32m     10\u001b[0m max_seq_len \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_dsets' is not defined"
     ]
    }
   ],
   "source": [
    "# import trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# train and evaluate using Evalb\n",
    "encodings = gen_dsets()\n",
    "results = {}\n",
    "max_seq_len = 128\n",
    "train_limit = None\n",
    "\n",
    "model_name = 'xlm-roberta-base'\n",
    "\n",
    "encoder_abs = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")    \n",
    "train_enc, mlt1 = encode_dset(encoder_abs, ptb_train[:train_limit] if train_limit else ptb_train)\n",
    "dev_enc,   mlt2   = encode_dset(encoder_abs, ptb_dev[:train_limit]   if train_limit else ptb_dev)\n",
    "dataset_abs  = generate_dataset_from_codelin(train_enc, dev_enc)\n",
    "\n",
    "encoder_rel = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "train_enc, mlt1 = encode_dset(encoder_rel, ptb_train[:train_limit] if train_limit else ptb_train)\n",
    "dev_enc,   mlt2   = encode_dset(encoder_rel, ptb_dev[:train_limit]   if train_limit else ptb_dev)\n",
    "dataset_rel  = generate_dataset_from_codelin(train_enc, dev_enc)\n",
    "\n",
    "tasks = [TokenClassification(\n",
    "            dataset = dataset_abs,\n",
    "            y = [\"target_1\", \"target_2\", \"target_3\"],\n",
    "            name = \"absolute\",\n",
    "            tokenizer_kwargs = frozendict(padding=\"max_length\", max_length = max_seq_len, truncation=True)\n",
    "        ),\n",
    "        TokenClassification(\n",
    "            dataset = dataset_rel,\n",
    "            y = [\"target_1\", \"target_2\", \"target_3\"],\n",
    "            name = \"relative\",\n",
    "            tokenizer_kwargs = frozendict(padding=\"max_length\", max_length = max_seq_len, truncation=True)\n",
    "        ),]\n",
    "\n",
    "model = MultiTaskModel(model_name, tasks)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"results/test\",\n",
    "    num_train_epochs = 2,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"f1\",\n",
    "    greater_is_better = True,\n",
    "    save_total_limit = 1,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "train_dataset = model.train_dataset\n",
    "trainer = MultiTaskTrainer(\n",
    "    model = model,\n",
    "    tasks = tasks,\n",
    "    args = training_args,\n",
    "    train_dataset = model.train_dataset,\n",
    "    eval_dataset = model.eval_dataset,\n",
    "    compute_metrics = None,\n",
    "    tokenizer = model.tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6a80f18e31b3afbf65f4c0ba16ab618b75d927e0f96f1f2ebf3c840526a5fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
