{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Sequence\n",
    "from datasets import ClassLabel\n",
    "\n",
    "from PYEVALB.scorer import Scorer\n",
    "from PYEVALB.summary import summary\n",
    "\n",
    "from codelin.models.const_tree import C_Tree\n",
    "from codelin.models.const_label import C_Label\n",
    "from codelin.models.linearized_tree import LinearizedTree\n",
    "from codelin.encs.constituent import *\n",
    "from codelin.utils.constants import *\n",
    "\n",
    "from frozendict import frozendict\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set logging level\n",
    "'''\n",
    "Train the models in multi-task learning fashion. To do this\n",
    "we will split the fields of the label and train different\n",
    "tasks according to this. After training, we will evaluate\n",
    "the decoded trees by re-joining the labels.\n",
    "'''\n",
    "\n",
    "ptb_path = \"~/Treebanks/const/PENN_TREEBANK/\"\n",
    "ptb_path = os.path.expanduser(ptb_path)\n",
    "\n",
    "with open(os.path.join(ptb_path,\"test.trees\")) as f:\n",
    "    ptb_test = [l.rstrip() for l in f.read().splitlines()]\n",
    "with open(os.path.join(ptb_path,\"dev.trees\")) as f:\n",
    "    ptb_dev = [l.rstrip() for l in f.read().splitlines()]\n",
    "with open(os.path.join(ptb_path,\"train.trees\")) as f:\n",
    "    ptb_train = [l.rstrip() for l in f.read().splitlines()]\n",
    "\n",
    "def get_n_labels(dsets, tar_field):\n",
    "    label_set = set()\n",
    "    for dset in dsets:\n",
    "        for labels in dset[tar_field]:\n",
    "            label_set.update(labels)\n",
    "    label_names = sorted(list(label_set))\n",
    "    return label_names, len(label_names)\n",
    "\n",
    "def generate_dataset_from_codelin(train_dset, dev_dset, test_dset=None):\n",
    "    dsets = [train_dset, dev_dset, test_dset] if test_dset else [train_dset, dev_dset]\n",
    "    \n",
    "    l1, nl1 = get_n_labels(dsets, \"target_1\")\n",
    "    l2, nl2 = get_n_labels(dsets, \"target_2\")\n",
    "    l3, nl3 = get_n_labels(dsets, \"target_3\")\n",
    "\n",
    "    train_dset = datasets.Dataset.from_dict(train_dset)\n",
    "    train_dset = train_dset.cast_column(\"target_1\", Sequence(ClassLabel(num_classes=nl1, names=l1)))\n",
    "    train_dset = train_dset.cast_column(\"target_2\", Sequence(ClassLabel(num_classes=nl2, names=l2)))\n",
    "    train_dset = train_dset.cast_column(\"target_3\", Sequence(ClassLabel(num_classes=nl3, names=l3)))\n",
    "\n",
    "    dev_dset = datasets.Dataset.from_dict(dev_dset)\n",
    "    dev_dset = dev_dset.cast_column(\"target_1\", Sequence(ClassLabel(num_classes=nl1, names=l1)))\n",
    "    dev_dset = dev_dset.cast_column(\"target_2\", Sequence(ClassLabel(num_classes=nl2, names=l2)))\n",
    "    dev_dset = dev_dset.cast_column(\"target_3\", Sequence(ClassLabel(num_classes=nl3, names=l3)))\n",
    "\n",
    "    if test_dset:\n",
    "        test_dset = datasets.Dataset.from_dict(test_dset)\n",
    "        test_dset = test_dset.cast_column(\"target_1\", Sequence(ClassLabel(num_classes=nl1, names=l1)))\n",
    "        test_dset = test_dset.cast_column(\"target_2\", Sequence(ClassLabel(num_classes=nl2, names=l2)))\n",
    "        test_dset = test_dset.cast_column(\"target_3\", Sequence(ClassLabel(num_classes=nl3, names=l3)))\n",
    "    \n",
    "        # Convert to Hugging Face DatasetDict format\n",
    "        dataset = datasets.DatasetDict({\n",
    "                \"train\": train_dset,\n",
    "                \"validation\": dev_dset,\n",
    "                \"test\": test_dset\n",
    "            })\n",
    "    else:\n",
    "        # Convert to Hugging Face DatasetDict format\n",
    "        dataset = datasets.DatasetDict({\n",
    "                \"train\": train_dset,\n",
    "                \"validation\": dev_dset\n",
    "            })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def encode_dset(encoder, dset):\n",
    "    encoded_trees = {\"tokens\":[], \"target_1\":[], \"target_2\":[], \"target_3\":[]}\n",
    "    max_len_tree = 0\n",
    "    for line in dset:\n",
    "        tree = C_Tree.from_string(line)\n",
    "        lin_tree = encoder.encode(tree)\n",
    "        encoded_trees[\"tokens\"].append([w for w in lin_tree.words])\n",
    "        \n",
    "        t1,t2,t3 = [],[],[]\n",
    "        for s1,s2,s3 in lin_tree.get_labels_splitted():\n",
    "            t1.append(s1)    \n",
    "            t2.append(s2)\n",
    "            t3.append(s3)\n",
    "            \n",
    "        encoded_trees[\"target_1\"].append(t1)\n",
    "        encoded_trees[\"target_2\"].append(t2)\n",
    "        encoded_trees[\"target_3\"].append(t3)\n",
    "        \n",
    "        max_len_tree = max(max_len_tree, len(lin_tree.words))\n",
    "    \n",
    "    \n",
    "    return encoded_trees, max_len_tree\n",
    "\n",
    "def gen_dsets():\n",
    "    encodings = []\n",
    "\n",
    "    # naive absolute encodings\n",
    "    a_enc     = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute\", \"encoder\":a_enc})\n",
    "    a_br_enc  = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_br\", \"encoder\":a_br_enc})\n",
    "    a_bl_enc  = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_bl\", \"encoder\":a_bl_enc})\n",
    "    ar_enc    = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_r\", \"encoder\":ar_enc})\n",
    "    ar_br_enc = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_r_br\", \"encoder\":ar_br_enc})\n",
    "    ar_bl_enc = C_NaiveAbsoluteEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_absolute_r_bl\", \"encoder\":ar_bl_enc})\n",
    "\n",
    "    # naive relative encodings\n",
    "    r_enc     = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative\", \"encoder\":r_enc})\n",
    "    r_br_enc  = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_br\", \"encoder\":r_br_enc})\n",
    "    r_bl_enc  = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_bl\", \"encoder\":r_bl_enc})\n",
    "    rr_enc    = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_r\", \"encoder\":rr_enc})\n",
    "    rr_br_enc = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_r_br\", \"encoder\":rr_br_enc})\n",
    "    rr_bl_enc = C_NaiveRelativeEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_relative_r_bl\", \"encoder\":rr_bl_enc})\n",
    "\n",
    "    # naive dynamic encodings\n",
    "    d_enc     = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic\", \"encoder\":d_enc})\n",
    "    d_br_enc  = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_br\", \"encoder\":d_br_enc})\n",
    "    d_bl_enc  = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=False, binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_bl\", \"encoder\":d_bl_enc})\n",
    "    dr_enc    = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_r\", \"encoder\":dr_enc})\n",
    "    dr_br_enc = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"R\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_r_br\", \"encoder\":dr_br_enc})\n",
    "    dr_bl_enc = C_NaiveDynamicEncoding(separator=\"[_]\", unary_joiner=\"[+]\", reverse=True,  binary=True,  binary_direction=\"L\",  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"naive_dynamic_r_bl\", \"encoder\":dr_bl_enc})\n",
    "\n",
    "    # gaps encodings\n",
    "    g_r_enc   = C_GapsEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary_direction=\"R\", binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"gaps_r\", \"encoder\":g_r_enc})\n",
    "    g_l_enc   = C_GapsEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary_direction=\"L\", binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"gaps_l\", \"encoder\":g_l_enc})\n",
    "\n",
    "    # tetra encodings\n",
    "    t_pr_enc  = C_Tetratag(separator=\"[_]\", unary_joiner=\"[+]\", mode='preorder',  binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"tetratag_preorder\", \"encoder\":t_pr_enc})\n",
    "    t_in_enc  = C_Tetratag(separator=\"[_]\", unary_joiner=\"[+]\", mode='inorder',   binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"tetratag_inorder\", \"encoder\":t_in_enc})\n",
    "    t_po_enc  = C_Tetratag(separator=\"[_]\", unary_joiner=\"[+]\", mode='postorder', binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"tetratag_postorder\", \"encoder\":t_po_enc})\n",
    "\n",
    "    # yuxtaposed encodings\n",
    "    j_enc   = C_JuxtaposedEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary=False, binary_direction=None, binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"juxtaposed\", \"encoder\":j_enc})\n",
    "    j_r_enc = C_JuxtaposedEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary=True, binary_direction='R',   binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"juxtaposed_r\", \"encoder\":j_r_enc})\n",
    "    j_l_enc = C_JuxtaposedEncoding(separator=\"[_]\", unary_joiner=\"[+]\", binary=True, binary_direction='L',   binary_marker=\"[b]\")\n",
    "    encodings.append({\"name\":\"juxtaposed_l\", \"encoder\":j_l_enc})\n",
    "\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TokenClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels = None, attention_mask = None, **kwargs):\n",
    "        sequence_output_dropout = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output_dropout)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            labels = labels.long()\n",
    "\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss,\n",
    "                    labels.view(-1),\n",
    "                    torch.tensor(loss_fct.ignore_index).type_as(labels),\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are processing is composed of \n",
    "- Task: Each of the datasets that we are employing \n",
    "- Subtasks: Each of the labels we are predicting for each word in the task dataset\n",
    "\n",
    "For example:\n",
    "\n",
    "- Task-1: The, dog, is, brown\n",
    "    - Subtask-1: DT, NOUN, VERB, ADJ\n",
    "    - Subtask-2: 1_NP, 2_VP, 1_ADJ, 1_NP\n",
    "    - Subtask-3: <_det, <//_root, >_whatever, //_mod\n",
    "<br><br>\n",
    "\n",
    "- Task-2: El, perro, es, marron\n",
    "    - Subtask-1: DT, NOUN, VERB, ADJ\n",
    "    - Subtask-2: 1_NP, 2_VP, 1_ADJ, 1_NP\n",
    "\n",
    "An example of this would be:\n",
    "\n",
    "```\n",
    "{\n",
    "    'naive_absolute_ptb': \n",
    "    {\n",
    "            'train': \n",
    "                    Dataset({\n",
    "                        features: ['tokens', 'target_1', 'target_2', 'input_ids', 'token_type_ids', 'attention_mask', 'task_ids'],\n",
    "                        num_rows: 256\n",
    "                    }), \n",
    "            'validation': \n",
    "                    Dataset({\n",
    "                        features: ['tokens', 'target_1', 'target_2', 'target_3', 'input_ids', 'token_type_ids', 'attention_mask', 'task_ids'],\n",
    "                        num_rows: 256\n",
    "                    })\n",
    "    },\n",
    "    'naive_absolute_en_ewt': \n",
    "    {\n",
    "            'train': \n",
    "                    Dataset({\n",
    "                        features: ['tokens', 'target_1', 'target_2', 'input_ids', 'token_type_ids', 'attention_mask', 'task_ids'],\n",
    "                        num_rows: 256\n",
    "                    }), \n",
    "            'validation': \n",
    "                    Dataset({\n",
    "                        features: ['tokens', 'target_1', 'target_2', 'input_ids', 'token_type_ids', 'attention_mask', 'task_ids'],\n",
    "                        num_rows: 256\n",
    "                    })\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Where we are training the fields of ```head_pos``` (taget_1) and ```dep_rel``` (target_2) for dependency parsing casted as sequence labeling for two different datasets (penn treebank and english web treebank). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import EvalPrediction\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import RandomSampler, WeightedRandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers.data.data_collator import InputDataClass\n",
    "from types import MappingProxyType\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import transformers\n",
    "from transformers.trainer_utils import EvalLoopOutput\n",
    "\n",
    "class DataLoaderWithTaskname:\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task = task_name\n",
    "        self.data_loader = data_loader\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            yield batch\n",
    "\n",
    "class NLPDataCollator:\n",
    "    def __init__(self, tasks):\n",
    "        self.tasks = tasks\n",
    "\n",
    "    def __call__(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
    "        features = [{k:v for k,v in x.items() if k!='task_ids'} for x in features]\n",
    "        return features\n",
    "    \n",
    "class MultitaskDataloader:\n",
    "    \"\"\"\n",
    "    Data loader that combines and samples from multiple single-task\n",
    "    data loaders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataloader_dict, p=1):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        N = max([len(x)**(1-p) for x in dataloader_dict.values()])\n",
    "        \n",
    "        f_p = lambda x: int(N*x**p)\n",
    "\n",
    "        self.num_batches_dict = {\n",
    "            task_name: f_p(len(dataloader))\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            f_p(len(dataloader.dataset)) for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and yield a batch from the respective\n",
    "        task Dataloader.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        \n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader)\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, encoder_name_or_path, tasks):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name_or_path)\n",
    "        tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name_or_path, **tokenizer_kwargs)\n",
    "        self.output_heads = nn.ModuleDict()\n",
    "        \n",
    "        for task in tasks:\n",
    "            task.set_tokenizer(self.tokenizer)\n",
    "            for subtask in task.y:\n",
    "                decoder = self._create_output_head(\n",
    "                    self.encoder.config.hidden_size, \n",
    "                    task.task_type, \n",
    "                    task.num_labels[subtask]\n",
    "                )\n",
    "                \n",
    "                self.output_heads[subtask] = decoder\n",
    "\n",
    "        self.processed_tasks = self.preprocess_tasks(tasks, self.tokenizer)\n",
    "        self.label_names = {task.name: task.label_names for task in tasks}\n",
    "        self.train_dataset = {self.processed_tasks[task.name]['train'] for task in tasks}\n",
    "        self.eval_dataset = {self.processed_tasks[task.name]['validation'] for task in tasks}\n",
    "    \n",
    "    def preprocess_tasks(self, tasks, tokenizer):      \n",
    "        features_dict = {}\n",
    "        for i, task in enumerate(tasks):\n",
    "            print(\"[*] Model preprocessing task\", task.name)\n",
    "            \n",
    "            if hasattr(task, 'processed_features') and tokenizer == task.tokenizer:\n",
    "                features_dict[task.name] = task.processed_features\n",
    "                continue\n",
    "            \n",
    "            for split in task.dataset:\n",
    "                task.index = task.dataset[split].index = i\n",
    "            \n",
    "            features_dict[task.name] = {}\n",
    "            for phase, phase_dataset in task.dataset.items():\n",
    "                phase_dataset.index = i\n",
    "\n",
    "                features_dict[task.name][phase] = phase_dataset.map(\n",
    "                    task.preprocess_function, \n",
    "                    batched = True,\n",
    "                    batch_size = 8,\n",
    "                    load_from_cache_file = True\n",
    "                )\n",
    "        return features_dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_output_head(encoder_hidden_size: int, task_type, n_labels):\n",
    "        if task_type == \"TokenClassification\":\n",
    "            print(\"[*] Creating TokenClassification head with\", n_labels, \"labels\")\n",
    "            return TokenClassificationHead(encoder_hidden_size, n_labels)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_ids = None, attention_mask = None, token_type_ids = None, position_ids = None,\n",
    "            head_mask = None, inputs_embeds = None, labels = None, task_ids = None, **kwargs):\n",
    "            print(\"FORWARD MODEL FORWARD MODEL FORWARD MODEL\")\n",
    "            # compute the transformer output\n",
    "            # this is never called?\n",
    "            outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "            sequence_output, pooled_output = outputs[:2]\n",
    "\n",
    "            print(\"3) Transformer has been forwarded\")\n",
    "            unique_task_ids_list = torch.unique(task_ids).tolist()\n",
    "\n",
    "            loss_list = []\n",
    "            logits = None\n",
    "            # print(\"Computing loss...\")\n",
    "            # print(\"task_ids\", task_ids)\n",
    "            print(\"==> I have to compute loss for the following tasks:\")\n",
    "            print(\"==>\", unique_task_ids_list)\n",
    "            for unique_task_id in unique_task_ids_list:\n",
    "                print(\"Task_id =\",unique_task_id)\n",
    "                ptc_train = self.processed_tasks['train']\n",
    "                target_cols = [col for col in ptc_train.features if col.startswith(\"target_\")]\n",
    "                print(\"target_cols =\", target_cols)\n",
    "\n",
    "                # for tc in target_cols:\n",
    "                #     print(\"Target Column =\",tc)\n",
    "                #     print(\"Labels =\",labels)\n",
    "                #     logits, task_loss = self.output_heads[str(unique_task_id)].forward(\n",
    "                #         sequence_output[task_id_filter],\n",
    "                #         pooled_output[task_id_filter],\n",
    "                #         labels = None if labels is None else labels[task_id_filter],\n",
    "                #         attention_mask=attention_mask[task_id_filter],\n",
    "                #     )\n",
    "\n",
    "                #     if labels is not None:\n",
    "                #         loss_list.append(task_loss)\n",
    "\n",
    "            # Loss averaged over all tasks\n",
    "            outputs = (logits, outputs[2:])\n",
    "            if loss_list:\n",
    "                loss = torch.stack(loss_list)\n",
    "                outputs = (loss.mean(),) + outputs\n",
    "\n",
    "            return outputs\n",
    "\n",
    "\n",
    "class MultiTaskTrainer(transformers.Trainer):\n",
    "    def __init__(self, tasks, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.p = 1\n",
    "        self.processed_tasks = self.model.processed_tasks\n",
    "        self.label_names = self.model.label_names\n",
    "        self.train_dataset = {\n",
    "            task: dataset[\"train\"]\n",
    "            for task, dataset in self.processed_tasks.items()\n",
    "        }\n",
    "        self.eval_dataset = {\n",
    "            task: dataset[\"validation\"]\n",
    "            for task, dataset in self.processed_tasks.items()\n",
    "        }\n",
    "        self.eval_dataset = MappingProxyType(self.eval_dataset)\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.pretrained_transformer = self.model.encoder\n",
    "        self.device = self.pretrained_transformer.device\n",
    "        self.data_collator = NLPDataCollator(tasks)\n",
    "        \n",
    "        print(\"[*] Init multitask trainer with tasks:\", self.processed_tasks)\n",
    "        print(\"[*] Label names are:\", self.label_names)\n",
    "           \n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        \n",
    "        train_sampler = (SequentialSampler(train_dataset) if self.args.local_rank == -1 else DistributedSampler(train_dataset))\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name = task_name,\n",
    "            data_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size = self.args.train_batch_size,\n",
    "                shuffle = False,\n",
    "                sampler = train_sampler,\n",
    "                collate_fn = self.data_collator.__call__,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        return MultitaskDataloader(\n",
    "            {\n",
    "                task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "                for task_name, task_dataset in self.train_dataset.items()\n",
    "            }, p = self.p,\n",
    "        )\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        return MultitaskDataloader(\n",
    "            {\n",
    "                task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "                for task_name, task_dataset in (\n",
    "                    eval_dataset if eval_dataset else self.eval_dataset\n",
    "                ).items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def evaluation_loop(self, dataloader: DataLoader, description: str, \n",
    "                        prediction_loss_only: bool | None = None, ignore_keys: List[str] | None = None, \n",
    "                        metric_key_prefix: str = \"eval_\") -> EvalLoopOutput:\n",
    "          \n",
    "        # this is useless?\n",
    "        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "\n",
    "        for step, inputs in enumerate(dataloader):            \n",
    "            loss, preds, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "            \n",
    "            for task, label_names in self.label_names.items():\n",
    "                preds_task = preds[task]\n",
    "                labels_task = labels[task]\n",
    "                \n",
    "                for label_name, labels_values in label_names.items():\n",
    "                    preds_tl  = preds_task[label_name]\n",
    "                    labels_tl = labels_task[label_name]\n",
    "                    \n",
    "                    eval_pred = EvalPrediction(\n",
    "                                predictions = preds_tl, \n",
    "                                label_ids   = labels_tl, \n",
    "                                inputs      = inputs)\n",
    "\n",
    "                    # compute metrics foreach head using the corresponding task eval_function\n",
    "                    # i copied the function from the task-specific class to this one\n",
    "                    metrics = self.compute_metrics_token_classification(eval_pred, label_name)\n",
    "                    metrics_eval = {}\n",
    "                    for metric in metrics.items():\n",
    "                        metrics_eval[metric_key_prefix + \"_\" + metric[0]] = metric[1]\n",
    "\n",
    "        return EvalLoopOutput(predictions=preds_tl, label_ids=labels_tl, metrics=metrics_eval, num_samples=len(self.eval_dataset))\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=[]):\n",
    "        if ignore_keys is None:\n",
    "            ignore_keys = []\n",
    "\n",
    "        loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "        loss = loss.mean().detach()\n",
    "        \n",
    "        logits_dict = {}\n",
    "        labels_dict = {}\n",
    "        for task_name, label_names in self.label_names.items():\n",
    "            logits_dict[task_name] = {}\n",
    "            labels_dict[task_name] = {}\n",
    "            for label_name in label_names:\n",
    "                logits_dict[task_name][label_name] = outputs[label_name]\n",
    "                logits_dict[task_name][label_name] = np.argmax(outputs[label_name].detach().cpu().numpy(), axis=2)\n",
    "                target_labels = []\n",
    "                for i in inputs:\n",
    "                    target_labels.append(i[label_name])\n",
    "                labels_dict[task_name][label_name] = torch.tensor(target_labels)\n",
    "        \n",
    "        return (loss, logits_dict, labels_dict)\n",
    "\n",
    "    def compute_metrics_token_classification(self, eval_pred, label_names):\n",
    "        predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "        \n",
    "        # for task in tasks\n",
    "        label_names_task = self.label_names['naive_absolute_n_commons']\n",
    "        true_labels = [\n",
    "            [label_names_task[label_names][int(l)] for l in label if l != -100] for label in labels\n",
    "        ]\n",
    "        \n",
    "        true_predictions = [\n",
    "            [label_names_task[label_names][p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        metric = evaluate.load(\"seqeval\")\n",
    "        all_metrics = metric.compute(\n",
    "            predictions = true_predictions, \n",
    "            references = true_labels\n",
    "        )\n",
    "        \n",
    "        meta = {\"name\": 'naive_absolute_n_commons', \"size\": len(predictions), \"index\": 0}\n",
    "        metrics = {k.replace(\"overall_\",\"\"):v for k,v in all_metrics.items() if \"overall\" in k}\n",
    "        \n",
    "        return {**metrics, **meta}      \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        keys = inputs[0].keys()\n",
    "\n",
    "        input_ids = torch.tensor([i['input_ids'] for i in inputs], device=self.args.device) if 'input_ids' in keys else None\n",
    "        attention_mask = torch.tensor([i['attention_mask'] for i in inputs], device=self.args.device) if 'attention_mask' in keys else None        \n",
    "        token_type_ids = torch.tensor([i['token_type_ids'] for i in inputs], device=self.args.device) if 'token_type_ids' in keys else None        \n",
    "        position_ids = torch.tensor([i['position_ids'] for i in inputs], device=self.args.device) if 'position_ids' in keys else None        \n",
    "        head_mask = torch.tensor([i['head_mask'] for i in inputs], device=self.args.device) if 'head_mask' in keys else None        \n",
    "        inputs_embeds = torch.tensor([i['inputs_embeds'] for i in inputs], device=self.args.device) if 'inputs_embeds' in keys else None\n",
    "        \n",
    "        outputs = self.pretrained_transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids,\n",
    "            head_mask = head_mask,\n",
    "            inputs_embeds = inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        loss_list = []\n",
    "        logits_list = {}\n",
    "        \n",
    "        for i, head in enumerate(self.model.output_heads.values()):\n",
    "            labels_name = f\"target_{i+1}\"\n",
    "            labels_i = torch.tensor([i[labels_name] for i in inputs], device=self.args.device)\n",
    "            logits, loss = head(sequence_output, pooled_output, labels=labels_i, attention_mask=attention_mask)\n",
    "            loss_list.append(loss)\n",
    "            logits_list[labels_name] = logits\n",
    "        \n",
    "        loss = torch.stack(loss_list)\n",
    "        return (loss, logits_list) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import funcy as fc\n",
    "import warnings\n",
    "from frozendict import frozendict as fdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TokenClassification:\n",
    "    task_type = \"TokenClassification\"\n",
    "    name: str = \"TokenClassificationTask\"\n",
    "    dataset: Dataset = None\n",
    "    metric:... = evaluate.load(\"seqeval\")\n",
    "    main_split: str = \"train\"\n",
    "    tokens: str = 'tokens'\n",
    "    y: str|list = 'target'\n",
    "    num_labels: int = None\n",
    "    label_names: dict = None\n",
    "    tokenizer_kwargs: fdict = fdict(padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _align_labels_with_tokens(labels, word_ids):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                new_labels.append(-100)\n",
    "\n",
    "            elif word_id != current_word:\n",
    "                current_word = word_id\n",
    "                label = -100 if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "            \n",
    "            else:\n",
    "                label = labels[word_id]\n",
    "                new_labels.append(label)\n",
    "        \n",
    "        return new_labels\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.label_names = {}\n",
    "        self.num_labels  = {}\n",
    "\n",
    "        for y in self.y:\n",
    "            target = self.dataset[self.main_split].features[y]\n",
    "            self.num_labels[y] = target.feature.num_classes\n",
    "            self.label_names[y] = target.feature.names if target.feature.names else [None]\n",
    "        \n",
    "        print(f\"[*] TokenClassificationTask loaded {self.task_type} task with {self.num_labels} labels\")\n",
    "        for k,v in self.label_names.items():\n",
    "            print(f\"      {k} labels: {v}\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return super().get_labels() or self.label_names\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.add_prefix_space = True\n",
    "        self.data_collator = DataCollatorForTokenClassification(\n",
    "            tokenizer = self.tokenizer\n",
    "        )\n",
    "\n",
    "    def preprocess_function(self, examples):\n",
    "        if examples[self.tokens] and type(examples[self.tokens][0]) == str:\n",
    "            unsqueeze, examples = True, {k:[v] for k,v in examples.items()}\n",
    "        \n",
    "        def get_len(outputs):\n",
    "            try:\n",
    "                return len(outputs[fc.first(outputs)])\n",
    "            except:\n",
    "                return 1\n",
    "        \n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            examples[self.tokens],\n",
    "            is_split_into_words=True,\n",
    "            **self.tokenizer_kwargs\n",
    "        )\n",
    "\n",
    "        for target_column in self.y:\n",
    "            all_labels = examples[target_column]\n",
    "            new_labels = []\n",
    "            \n",
    "            for i, labels in enumerate(all_labels):\n",
    "                word_ids = tokenized_inputs.word_ids(i)\n",
    "                new_labels.append(self._align_labels_with_tokens(labels, word_ids))\n",
    "            \n",
    "            tokenized_inputs[target_column] = new_labels        \n",
    "            tokenized_inputs['task_ids'] = [self.index]*get_len(tokenized_inputs)\n",
    "\n",
    "        return tokenized_inputs       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66db374034449bca7c99691bca4638e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fac049d85041a593a35dc9c1caddb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b2c111d8db457d974d98402de78758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6265345f854c32b0a1cfdbed726434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb17aa6ecc546daa6b85830812d0039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75638d2e33d410ea79808c52fb3e2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] TokenClassificationTask loaded TokenClassification task with {'target_1': 29, 'target_2': 40, 'target_3': 21} labels\n",
      "      target_1 labels: ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '4', '5', '6', '7', '8', '9']\n",
      "      target_2 labels: ['ADJP', 'ADJP[+]ADJP', 'ADJP[+]QP', 'ADVP', 'CONJP', 'FRAG', 'FRAG[+]ADJP', 'FRAG[+]NP', 'LST', 'NAC', 'NP', 'NP[+]NP', 'NP[+]QP', 'NP[+]VBN', 'NX', 'PP', 'PRN', 'PRN[+]S', 'QP', 'S', 'SBAR', 'SBARQ', 'SBAR[+]S', 'SBAR[+]SINV', 'SBAR[+]S[+]VP', 'SINV', 'SQ', 'SQ[+]VP', 'S[+]ADJP', 'S[+]ADVP', 'S[+]NP', 'S[+]PP', 'S[+]VP', 'UCP', 'VP', 'VP[+]VP', 'WHADJP', 'WHADVP', 'WHNP', 'WHPP']\n",
      "      target_3 labels: ['-NONE-', 'ADJP', 'ADJP[+]ADJP', 'ADVP', 'ADVP[+]ADVP', 'FRAG[+]ADJP', 'FRAG[+]PP', 'INTJ', 'NP', 'NP[+]NP', 'NX', 'PP', 'PRT', 'SQ[+]VP', 'S[+]ADJP', 'S[+]NP', 'S[+]VP', 'VP', 'WHADVP', 'WHNP', 'X']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Creating TokenClassification head with 29 labels\n",
      "[*] Creating TokenClassification head with 40 labels\n",
      "[*] Creating TokenClassification head with 21 labels\n",
      "[*] Model preprocessing task naive_absolute_n_commons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d27ce197f54f89a5208c77214279f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cb4070316f4d4dabcb000ccff7a2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Init multitask trainer with tasks: {'naive_absolute_n_commons': {'train': Dataset({\n",
      "    features: ['tokens', 'target_1', 'target_2', 'target_3', 'input_ids', 'attention_mask', 'task_ids'],\n",
      "    num_rows: 512\n",
      "}), 'validation': Dataset({\n",
      "    features: ['tokens', 'target_1', 'target_2', 'target_3', 'input_ids', 'attention_mask', 'task_ids'],\n",
      "    num_rows: 512\n",
      "})}}\n",
      "[*] Label names are: {'naive_absolute_n_commons': {'target_1': ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '4', '5', '6', '7', '8', '9'], 'target_2': ['ADJP', 'ADJP[+]ADJP', 'ADJP[+]QP', 'ADVP', 'CONJP', 'FRAG', 'FRAG[+]ADJP', 'FRAG[+]NP', 'LST', 'NAC', 'NP', 'NP[+]NP', 'NP[+]QP', 'NP[+]VBN', 'NX', 'PP', 'PRN', 'PRN[+]S', 'QP', 'S', 'SBAR', 'SBARQ', 'SBAR[+]S', 'SBAR[+]SINV', 'SBAR[+]S[+]VP', 'SINV', 'SQ', 'SQ[+]VP', 'S[+]ADJP', 'S[+]ADVP', 'S[+]NP', 'S[+]PP', 'S[+]VP', 'UCP', 'VP', 'VP[+]VP', 'WHADJP', 'WHADVP', 'WHNP', 'WHPP'], 'target_3': ['-NONE-', 'ADJP', 'ADJP[+]ADJP', 'ADVP', 'ADVP[+]ADVP', 'FRAG[+]ADJP', 'FRAG[+]PP', 'INTJ', 'NP', 'NP[+]NP', 'NX', 'PP', 'PRT', 'SQ[+]VP', 'S[+]ADJP', 'S[+]NP', 'S[+]VP', 'VP', 'WHADVP', 'WHNP', 'X']}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dad55ff59384ff9954a25b196c3ab23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Evaluation_loop\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n",
      "[*] Prediction Step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m\n\u001b[1;32m     49\u001b[0m train_dataset \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain_dataset\n\u001b[1;32m     50\u001b[0m trainer \u001b[39m=\u001b[39m MultiTaskTrainer(\n\u001b[1;32m     51\u001b[0m     model \u001b[39m=\u001b[39m model,\n\u001b[1;32m     52\u001b[0m     tasks \u001b[39m=\u001b[39m tasks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     tokenizer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtokenizer\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/transformers/trainer.py:1660\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1657\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1658\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1659\u001b[0m )\n\u001b[0;32m-> 1660\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1661\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1662\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1663\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1664\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1665\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/transformers/trainer.py:2017\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2017\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2019\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   2020\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2021\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/transformers/trainer.py:2283\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2281\u001b[0m             metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2282\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2283\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/transformers/trainer.py:2988\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2985\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2987\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2988\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2989\u001b[0m     eval_dataloader,\n\u001b[1;32m   2990\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2991\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2992\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2993\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2994\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2995\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2996\u001b[0m )\n\u001b[1;32m   2998\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2999\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "Cell \u001b[0;32mIn[3], line 276\u001b[0m, in \u001b[0;36mMultiTaskTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    269\u001b[0m eval_pred \u001b[39m=\u001b[39m EvalPrediction(\n\u001b[1;32m    270\u001b[0m             predictions \u001b[39m=\u001b[39m preds_tl, \n\u001b[1;32m    271\u001b[0m             label_ids   \u001b[39m=\u001b[39m labels_tl, \n\u001b[1;32m    272\u001b[0m             inputs      \u001b[39m=\u001b[39m inputs)\n\u001b[1;32m    274\u001b[0m \u001b[39m# compute metrics foreach head using the corresponding task eval_function\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m# i copied the function from the task-specific class to this one\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics_token_classification(eval_pred, label_name)\n\u001b[1;32m    277\u001b[0m metrics_eval \u001b[39m=\u001b[39m {}\n\u001b[1;32m    278\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[3], line 320\u001b[0m, in \u001b[0;36mMultiTaskTrainer.compute_metrics_token_classification\u001b[0;34m(self, eval_pred, label_names)\u001b[0m\n\u001b[1;32m    312\u001b[0m true_labels \u001b[39m=\u001b[39m [\n\u001b[1;32m    313\u001b[0m     [label_names_task[label_names][\u001b[39mint\u001b[39m(l)] \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m label \u001b[39mif\u001b[39;00m l \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m labels\n\u001b[1;32m    314\u001b[0m ]\n\u001b[1;32m    316\u001b[0m true_predictions \u001b[39m=\u001b[39m [\n\u001b[1;32m    317\u001b[0m     [label_names_task[label_names][p] \u001b[39mfor\u001b[39;00m (p, l) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(prediction, label) \u001b[39mif\u001b[39;00m l \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m]\n\u001b[1;32m    318\u001b[0m     \u001b[39mfor\u001b[39;00m prediction, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(predictions, labels)\n\u001b[1;32m    319\u001b[0m ]\n\u001b[0;32m--> 320\u001b[0m metric \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mseqeval\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    321\u001b[0m all_metrics \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mcompute(\n\u001b[1;32m    322\u001b[0m     predictions \u001b[39m=\u001b[39m true_predictions, \n\u001b[1;32m    323\u001b[0m     references \u001b[39m=\u001b[39m true_labels\n\u001b[1;32m    324\u001b[0m )\n\u001b[1;32m    326\u001b[0m meta \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mnaive_absolute_n_commons\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlen\u001b[39m(predictions), \u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m}\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/evaluate/loading.py:731\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a `evaluate.EvaluationModule`.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[39m    `evaluate.EvaluationModule`\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    730\u001b[0m download_mode \u001b[39m=\u001b[39m DownloadMode(download_mode \u001b[39mor\u001b[39;00m DownloadMode\u001b[39m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 731\u001b[0m evaluation_module \u001b[39m=\u001b[39m evaluation_module_factory(\n\u001b[1;32m    732\u001b[0m     path, module_type\u001b[39m=\u001b[39;49mmodule_type, revision\u001b[39m=\u001b[39;49mrevision, download_config\u001b[39m=\u001b[39;49mdownload_config, download_mode\u001b[39m=\u001b[39;49mdownload_mode\n\u001b[1;32m    733\u001b[0m )\n\u001b[1;32m    734\u001b[0m evaluation_cls \u001b[39m=\u001b[39m import_main_class(evaluation_module\u001b[39m.\u001b[39mmodule_path)\n\u001b[1;32m    735\u001b[0m evaluation_instance \u001b[39m=\u001b[39m evaluation_cls(\n\u001b[1;32m    736\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[1;32m    737\u001b[0m     process_id\u001b[39m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs,\n\u001b[1;32m    744\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/evaluate/loading.py:633\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mfor\u001b[39;00m current_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcomparison\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmeasurement\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    632\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m         \u001b[39mreturn\u001b[39;00m HubEvaluationModuleFactory(\n\u001b[1;32m    634\u001b[0m             \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mevaluate-\u001b[39;49m\u001b[39m{\u001b[39;49;00mcurrent_type\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    635\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    636\u001b[0m             download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    637\u001b[0m             download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m    638\u001b[0m             dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path,\n\u001b[1;32m    639\u001b[0m         )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m    640\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/evaluate/loading.py:462\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.__init__\u001b[0;34m(self, name, module_type, revision, download_config, download_mode, dynamic_modules_path)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamic_modules_path \u001b[39m=\u001b[39m dynamic_modules_path\n\u001b[1;32m    461\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 462\u001b[0m increase_load_count(name, resource_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmetric\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/evaluate/loading.py:135\u001b[0m, in \u001b[0;36mincrease_load_count\u001b[0;34m(name, resource_type)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mHF_EVALUATE_OFFLINE \u001b[39mand\u001b[39;00m config\u001b[39m.\u001b[39mHF_UPDATE_DOWNLOAD_COUNTS:\n\u001b[1;32m    134\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m         head_hf_s3(name, filename\u001b[39m=\u001b[39;49mname \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.py\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataset\u001b[39m=\u001b[39;49m(resource_type \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    136\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/evaluate/utils/file_utils.py:93\u001b[0m, in \u001b[0;36mhead_hf_s3\u001b[0;34m(identifier, filename, use_cdn, dataset, max_retries)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhead_hf_s3\u001b[39m(\n\u001b[1;32m     91\u001b[0m     identifier: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, use_cdn\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dataset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_retries\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[requests\u001b[39m.\u001b[39mResponse, \u001b[39mException\u001b[39;00m]:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m http_head(\n\u001b[1;32m     94\u001b[0m         hf_bucket_url(identifier\u001b[39m=\u001b[39;49midentifier, filename\u001b[39m=\u001b[39;49mfilename, use_cdn\u001b[39m=\u001b[39;49muse_cdn, dataset\u001b[39m=\u001b[39;49mdataset),\n\u001b[1;32m     95\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m     96\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/evaluate/utils/file_utils.py:427\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    425\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    426\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 427\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    428\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    429\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    430\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    431\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    432\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    433\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[1;32m    434\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    435\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    436\u001b[0m )\n\u001b[1;32m    437\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/evaluate/utils/file_utils.py:356\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    354\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    355\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 356\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    357\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# train and evaluate using Evalb\n",
    "encodings = gen_dsets()\n",
    "results = {}\n",
    "max_seq_len = 128\n",
    "train_limit = 512\n",
    "# model_name = \"bert-base-cased\"\n",
    "model_name = 'xlm-roberta-base'\n",
    "\n",
    "# probably this could be done in parallel\n",
    "for enc in encodings[:1]:\n",
    "    results_df = pd.DataFrame(columns=[\"encoding\", \"recall\", \"precision\", \"f1\", \"n_labels\"])\n",
    "    encoder = enc[\"encoder\"]\n",
    "    \n",
    "    train_enc, mlt1 = encode_dset(encoder, ptb_train[:train_limit] if train_limit else ptb_train)\n",
    "    dev_enc,   mlt2   = encode_dset(encoder, ptb_dev[:train_limit]   if train_limit else ptb_dev)\n",
    "    dataset  = generate_dataset_from_codelin(train_enc, dev_enc)\n",
    "\n",
    "    tasks = [TokenClassification(\n",
    "                dataset = dataset,\n",
    "                y = [\"target_1\", \"target_2\", \"target_3\"],\n",
    "                name = enc[\"name\"]+\"_n_commons\",\n",
    "                tokenizer_kwargs = frozendict(padding=\"max_length\", max_length = max_seq_len, truncation=True)\n",
    "            )]\n",
    "    \n",
    "    model = MultiTaskModel(model_name, tasks)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = f\"results/{enc['name']}\",\n",
    "        num_train_epochs = 2,\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size = 8,\n",
    "        warmup_steps = 500,\n",
    "        weight_decay = 0.01,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"f1\",\n",
    "        greater_is_better = True,\n",
    "        save_total_limit = 1,\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    train_dataset = model.train_dataset\n",
    "    trainer = MultiTaskTrainer(\n",
    "        model = model,\n",
    "        tasks = tasks,\n",
    "        args = training_args,\n",
    "        train_dataset = model.train_dataset,\n",
    "        eval_dataset = model.eval_dataset,\n",
    "        compute_metrics = None,\n",
    "        tokenizer = model.tokenizer\n",
    "    )\n",
    "    \n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6a80f18e31b3afbf65f4c0ba16ab618b75d927e0f96f1f2ebf3c840526a5fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
