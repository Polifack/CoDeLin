## DONE:

[x] Implement tetra-tagging.
[x] Implement binarization for all constituent encodings.
[x] Implement incremental for all constituent encodings.
[x] Evaluate incremental and binary encodings in PTB.
[x] Search for NNE datasets
[x] Implement NER 2 Trees
[x] Implement NER 2 BIO/BILOU
[x] Implement postorder and inorder tetratagging. Currently we have only inorder.
[x] Sign up en https://altausuarios.cesga.es/
[x] Implement +G,-G encoding: No funciona; se necesitan mas labels
[x] Implement gaps encoding with n labels (e.g. instead of encoding '-G' for all gaps encode -G1, -G2...?)
[x] Attach juxtaposed transition based constituent parsing (as much transitions as words?) [link](https://arxiv.org/pdf/2010.14568.pdf)
[x] Develop multi task learning system for several encodings
[x] Check the gaps encoding with both binary directions and reverse
[x] Check the tetratag with several orders with both binary directions and reverse
[x] Check the juxtaposed encoding with reverse and binarization
[x] Implement a better constituent stats extraction
[x] Implement multi-task learning for each field of the labels
[x] Write introduction and related work for paper

## WIP

[ ] Semantic Parsing? https://aclanthology.org/2022.starsem-1.19.pdf
	- Discourse Representation Structure
[ ] Could seq_lbl do something related w/ HPSG?

[ ] MTL? f1: 81.40267556955776 para el absolute a un solo epoch! However for ~20 epochs the training takes 8 hours...

[ ] Train with PTB for all encodings. Use ROBERTA.
[ ] Train with PTB for all encodings using mtl for each field of the labels. Use ROBERTA.

[ ] Check Treebanks branching factor
[ ] Check number of labels generated by trees of different labels
[ ] Once the models are trained, extract stats per tree depth (e.g. f1 score for trees with depth<=3)
[ ] Take advantage of the MTL system to get a encoding with several outputs and use them both to get a better decoding

[ ] Check the alignment function (something is not working as intended?)


## BACKLOG

[ ] Get stats of NER trees (e.g. average-depth, number of trees with depth higher than n...)

[ ] Train the BIO and BILOU models for GENIA

[ ] Evaluate all models of NER

[ ] Discontinous Parsing Implementation

[ ] Implement decoding heuristics for tree2ner 
	=> NER trees must have only one root node
	=> Check all open entities are closed
	=> Check max entity length

[ ] Find evaluation script for NER:
	=> Evaluate by checking complete entities
	=> Evaluate by checking correct ner tokens

[ ] Multi task learning with NER (parse tree + ner tree)

[ ] Multi task learning with sentiment analysis (parse tree + sentiment indicator)
	=> We could use the Stanford sentiment treebank

[ ] Check semantic dependency parsing
		=> We could employ the planar encoding for this task (instead of *only* one outgoing arc, we could have several arcs)
		=> biaffine attention? (https://arxiv.org/pdf/1611.01734.pdf)

[ ] Abstract meaning representation (AMR) parsing
		=> We could employ a seq2seq model to turn words into concepts
		=> We could employ Sequence Labeling after that

[ ] Check for models quantification in the MTL setup

## Deadlines
- AACL  23/05
- EMNLP 23/06
