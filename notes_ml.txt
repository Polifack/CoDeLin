librerias consideradas:
------------------------

-> keras
high level:
-> ncrf++ (hacer los modelos enchufables a cualquier modelo) https://github.com/jipipesutd/NCRFpp
-> stanza
-> bert | roberta
-> https://github.com/machamp-nlp/machamp (nueva; deja hacer multitask)

hugginface? contextualizar palabras?

------------------------

multitask learnign (+1 lbl a cada palabra)
    -> una parte del modelo predice la parte xi de la label y la otra la li
    -> ventaja para scarcity de labels
    -> ventaja para NER

------------------------
estadisticas a medir:

    Dependencias: LAS UAS (UD -> tiene script oficial [!]) CONLL-2007 (?) 
        http://universaldependencies.org/conll17/evaluation.html
        https://universaldependencies.org/conll18/evaluation.html [mas nuevo]

    Constituyentes: el script de gold que tenias antes, ejecutar con el fichero collins

----------------

A la hora de entrenamiento calcular datos de precision y velocidad 

----------------
Dependencias
----------------

Encoder:
--------
Unless otherwise specified, the input token at a given time step is the concatenation of
a word, PoS tag, and another word embedding learned through a character LSTM.
We consider stacked BiLSTMs, where the output hmi of the m BiLSTM layer is fed as input to the m+1 layer

Decoder:
--------
We use a feed-forward network, which is fed the output of the last BiLSTM. The output
is computed as P (yi|hi) = softmax(W Â· hi + b).







----------------
Constituyentes
----------------
Baseline techniques:
    Conditional Random Fields
    MultiLayer Perceptron

Dada una frase [w1, w2, ... , wn] la entrada a la bilstm es [v1, v2, ... , vn]
donde el vector vi esta formado por

        -> palabra wi
        -> postag pi
        -> word embedding ch

El word embeding ch se obtiene de una character embeding layer (tambien basada en lstm)
